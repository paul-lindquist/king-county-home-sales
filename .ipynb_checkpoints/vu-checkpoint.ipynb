{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](./data/home-sales-shutterstock-295804091-1068x601.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Home Sales\n",
    "**Authors:** [Jerry Vasquez](https://www.linkedin.com/in/jerry-vasquez-832b71224/), [Paul Lindquist](https://www.linkedin.com/in/paul-lindquist/), [Vu Brown](https://www.linkedin.com/in/austin-brown-b5211384/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "***\n",
    "This is our overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "***\n",
    "This is our business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "***\n",
    "This is where the data is sourced from with focuses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "***\n",
    "Descriptive analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "***\n",
    "Notes on EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from collections import Counter\n",
    "import folium\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "sns.set_theme(palette='magma_r')\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder, MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "pd.set_option('display.max_rows', 500) # Allows Jupyter Notebook to expand how much data is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with highest correlation to price\n",
    "price_corr = df.corr()['price'].map(abs).sort_values(ascending=False)\n",
    "price_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(data=df, x='long', y='lat', hue='price', palette='magma_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning, Preparation, Feature Engineering for Inferential Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to identify duplicates\n",
    "def determine_dupes(series):\n",
    "    series_vcs = pd.Series(series.value_counts())\n",
    "    series_dupes = [series_vcs.index[index] for index in range(len(series_vcs)) if series_vcs.values[index] > 1]\n",
    "    print(\"Amount of unique duplicates: \" + str(len(series_dupes)))\n",
    "    print(\"Total amount of duplicates: \" + str(series_vcs.values[0:len(series_dupes)].sum()))\n",
    "    \n",
    "    return series_vcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run duplicates function for 'id' series\n",
    "determine_dupes(infer_df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(infer_df.loc[infer_df.id == 795000620])\n",
    "# display(infer_df[infer_df.duplicated(subset=['id'], keep=False)].head(20))\n",
    "# display(infer_df[infer_df.duplicated(subset=['id'], keep='first')].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop duplicates found within 'id' series\n",
    "infer_df = infer_df.drop_duplicates(subset=['id'], keep='last')\n",
    "infer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Consider droping duplicated based upon latitude and longitude\n",
    "# infer_df[infer_df.duplicated(subset=['lat','long'], keep=False)].sort_values('lat')\n",
    "# infer_df = infer_df.drop_duplicates(subset=['lat', 'long'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make features more workable by dealing with missing/bunk values and changing series from objects to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN/?/missing values with 0, None or No for respective series\n",
    "# Also change object series to integer via astype function\n",
    "infer_df.yr_renovated = infer_df.yr_renovated.fillna(0)\n",
    "infer_df.yr_renovated = infer_df.yr_renovated.astype('int64')\n",
    "\n",
    "infer_df.view = infer_df.view.fillna('NONE')\n",
    "\n",
    "infer_df.waterfront = infer_df.waterfront.fillna('NO')\n",
    "\n",
    "infer_df.loc[infer_df.sqft_basement == '?', 'sqft_basement'] = 0.0\n",
    "infer_df.sqft_basement = infer_df.sqft_basement.astype('float64').astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "infer_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df.grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'grade' series objects to corresponding integers\n",
    "infer_df.grade = pd.to_numeric(infer_df.grade.map(lambda x: x.split()[0]))\n",
    "infer_df.grade.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'condition' series objects to corresponding integers\n",
    "# Integer values from https://info.kingcounty.gov/assessor/esales/Glossary.aspx\n",
    "infer_df['condition'].replace('Poor', 1, inplace=True)\n",
    "infer_df['condition'].replace('Fair', 2, inplace=True)\n",
    "infer_df['condition'].replace('Average', 3, inplace=True)\n",
    "infer_df['condition'].replace('Good', 4, inplace=True)\n",
    "infer_df['condition'].replace('Very Good', 5, inplace=True)\n",
    "infer_df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'waterfront' series objects to integers\n",
    "lb_make = LabelEncoder()\n",
    "infer_df['waterfront'] = lb_make.fit_transform(infer_df['waterfront'])\n",
    "infer_df.waterfront.value_counts()\n",
    "# 0:NO, 1:YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df.view.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'view' series objects to corresponding integers\n",
    "# Integer values mirrored from 'condition' series\n",
    "infer_df['view'].replace('NONE', 0, inplace=True)\n",
    "infer_df['view'].replace('FAIR', 2, inplace=True)\n",
    "infer_df['view'].replace('AVERAGE', 3, inplace=True)\n",
    "infer_df['view'].replace('GOOD', 4, inplace=True)\n",
    "infer_df['view'].replace('EXCELLENT', 5, inplace=True)\n",
    "infer_df.view.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'date' series to datetime data type (may not be needed)\n",
    "infer_df['date'] = pd.to_datetime(infer_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(infer_df.info())\n",
    "display(infer_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and drop outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df.price.hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.price.mean()+3*infer_df.price.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.price < (infer_df.price.mean() + 3*infer_df.price.std())]\n",
    "display(infer_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bedroom outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_df.loc[infer_df.bedrooms == 10].sort_values('sqft_living', ascending=False).head(20)\n",
    "\n",
    "# # 33 bedrooms for a 1620 sqft house is a mistake. \n",
    "# # 11 bedrooms for a 3000 sqft house is also a mistake.\n",
    "# # We'll drop those records\n",
    "# infer_df.drop(infer_df.loc[infer_df['bedrooms']==33].index, inplace=True)\n",
    "# infer_df.drop(infer_df.loc[infer_df['bedrooms']==11].index, inplace=True)\n",
    "# display(infer_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine number of bedrooms for outliers\n",
    "infer_df.bedrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.bedrooms.mean()-3*infer_df.bedrooms.std())\n",
    "print(infer_df.bedrooms.mean()+3*infer_df.bedrooms.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[(infer_df.bedrooms < (infer_df.bedrooms.mean() + 3*infer_df.bedrooms.std()))]\n",
    "display(infer_df.info())\n",
    "infer_df.bedrooms.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bathroom outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df.bathrooms.value_counts()\n",
    "# infer_df.loc[infer_df.bathrooms == 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.bathrooms.mean()-3*infer_df.bathrooms.std())\n",
    "print(infer_df.bathrooms.mean()+3*infer_df.bathrooms.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[(infer_df.bathrooms < (infer_df.bathrooms.mean() + 3*infer_df.bathrooms.std()))]\n",
    "display(infer_df.info())\n",
    "infer_df.bathrooms.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Floors outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df.floors.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.floors.mean()-3*infer_df.floors.std())\n",
    "print(infer_df.floors.mean()+3*infer_df.floors.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[(infer_df.floors < (infer_df.floors.mean() + 3*infer_df.floors.std()))]\n",
    "display(infer_df.info())\n",
    "infer_df.floors.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sq. Ft. Living outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_living, infer_df.price)\n",
    "axs.axvline(infer_df.sqft_living.mean()+3.5*infer_df.sqft_living.std())\n",
    "axs.set_title('sqft_living');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.sqft_living.mean()-3*infer_df.sqft_living.std())\n",
    "print(infer_df.sqft_living.mean()+3.5*infer_df.sqft_living.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[(infer_df.sqft_living < (infer_df.sqft_living.mean() + 3.5*infer_df.sqft_living.std()))]\n",
    "display(infer_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_living, infer_df.price)\n",
    "axs.set_title('sqft_living');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sq. Ft. Lot outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20,10))\n",
    "axs.scatter(infer_df.sqft_lot, infer_df.price)\n",
    "axs.axvline(infer_df.sqft_lot.mean()+3*infer_df.sqft_lot.std())\n",
    "axs.set_title('sqft_lot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.sqft_lot.mean()-4*infer_df.sqft_lot.std())\n",
    "print(infer_df.sqft_lot.mean()+3*infer_df.sqft_lot.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[(infer_df.sqft_lot < (infer_df.sqft_lot.mean() + 3*infer_df.sqft_lot.std()))]\n",
    "display(infer_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20,10))\n",
    "axs.scatter(infer_df.sqft_lot, infer_df.price)\n",
    "axs.set_title('sqft_lot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sq. Ft. Above outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_above, infer_df.price)\n",
    "axs.axvline(infer_df.sqft_above.mean()+3*infer_df.sqft_above.std())\n",
    "axs.set_title('sqft_above');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.sqft_above.mean()-3*infer_df.sqft_above.std())\n",
    "print(infer_df.sqft_above.mean()+3*infer_df.sqft_above.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[(infer_df.sqft_above < (infer_df.sqft_above.mean() + 3*infer_df.sqft_above.std()))]\n",
    "display(infer_df.info())\n",
    "infer_df.sqft_above.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_above, infer_df.price)\n",
    "axs.set_title('sqft_above');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sq. Ft. Basement outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_basement, infer_df.price)\n",
    "axs.axvline(infer_df.sqft_basement.mean()+3*infer_df.sqft_basement.std())\n",
    "axs.set_title('sqft_basement');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.sqft_basement.mean()-3*infer_df.sqft_basement.std())\n",
    "print(infer_df.sqft_basement.mean()+3*infer_df.sqft_basement.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infer_df = infer_df[(infer_df.sqft_basement < (infer_df.sqft_basement.mean() + 3*infer_df.sqft_basement.std()))]\n",
    "display(infer_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_basement, infer_df.price)\n",
    "axs.set_title('sqft_basement');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "***\n",
    "Notes on models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model training and testing data\n",
    "\n",
    "# Trial 1\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'condition', 'sqft_above',\n",
    "#                            'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "#                            'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 2 (Difference from Trial 1--> Dropped grade)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'condition', 'grade', 'sqft_above',\n",
    "#                            'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', \n",
    "#                            'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 3 (Difference from Trial 2--> Dropped bedrooms and bathrooms)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'bedrooms', 'bathrooms', 'condition',\n",
    "#                            'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',\n",
    "#                            'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 4 (Difference from Trial 3--> Dropped waterfront)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'bedrooms', 'bathrooms', 'waterfront',\n",
    "#                            'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built',\n",
    "#                            'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 5 (Difference from Trial 1--> Dropped waterfront)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'waterfront', 'condition', 'sqft_above',\n",
    "#                            'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n",
    "#                            'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 6 (Difference from Trial 1--> Dropped waterfront, view, grade; Added sqft_above, sqft_basement)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'waterfront', 'view', 'condition', 'grade',\n",
    "#                            'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 7 (Difference from Trial 1--> Added condition, sqft_above, sqft_basement)\n",
    "X = infer_df.drop(columns=['price', 'id', 'date', 'yr_built', 'yr_renovated',\n",
    "                           'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "y = infer_df.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show feature correlation of training data\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "corr = train_data.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "            ax=ax,annot=True, cbar_kws={\"label\": \"Correlation\",\n",
    "                                        \"orientation\": \"horizontal\",\n",
    "                                        \"pad\": .2, \"extend\": \"both\"});\n",
    "\n",
    "# Trial 1 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "# sqft_living & grade\n",
    "\n",
    "# Trial 2 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "\n",
    "# Trials 3, 4 - Multicollinearity Concerns:\n",
    "# sqft_living & price\n",
    "\n",
    "# Trial 5 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "# sqft_living & grade\n",
    "\n",
    "# Trial 6 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "# sqft_living & sqft_above\n",
    "\n",
    "# Trial 7 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "# sqft_living & grade\n",
    "# sqft_living & sqft_above\n",
    "# sqft_above & grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show scatter plots of training data compared to target\n",
    "\n",
    "# Trials 1, 2, 5, 6\n",
    "# fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(16, 10))\n",
    "\n",
    "# Trials 3, 4\n",
    "# fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(16, 7))\n",
    "\n",
    "# Trial 7 \n",
    "fig, axes = plt.subplots(ncols=3, nrows=4, figsize=(16, 10))\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "for index, col in enumerate(X_train.columns):\n",
    "    ax = axes[index//3][index%3]\n",
    "    ax.scatter(X_train[col], y_train) #, alpha=0.2)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('price')\n",
    "\n",
    "    \n",
    "# Trial 1\n",
    "# fig.delaxes(axes[2][2])\n",
    "\n",
    "# Trials 2, 5, 6\n",
    "# fig.delaxes(axes[2][1])\n",
    "# fig.delaxes(axes[2][2])\n",
    "\n",
    "# Trial 3\n",
    "# fig.delaxes(axes[1][2])\n",
    "\n",
    "# Trial 4\n",
    "# fig.delaxes(axes[1][1])\n",
    "# fig.delaxes(axes[1][2])\n",
    "\n",
    "# Trial 7\n",
    "fig.delaxes(axes[3][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model with DummyRegressor\n",
    "baseline = DummyRegressor()\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first model with highested correlated feature ('sqft_living')\n",
    "\n",
    "# Trials 1, 5, 7\n",
    "most_correlated_feature = 'grade'\n",
    "\n",
    "# Trial 2, 3, 4, 6\n",
    "# most_correlated_feature = 'sqft_living'\n",
    "\n",
    "first_model = LinearRegression()\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    "\n",
    "first_scores = cross_validate(estimator=first_model,\n",
    "                                 X=X_train[[most_correlated_feature]],\n",
    "                                 y=y_train, return_train_score=True,\n",
    "                                 cv=splitter)\n",
    "\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())\n",
    "\n",
    "# Trials 1, 5, 7:\n",
    "# First Model\n",
    "# Train score:  0.4026275172522893\n",
    "# Validation score:  0.39002239307457764\n",
    "\n",
    "# Trials 2, 3, 4, 6:\n",
    "# First Model\n",
    "# Train score:  0.3824853370125143\n",
    "# Validation score:  0.37867372602330196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients of first model\n",
    "sm.OLS(y_train, sm.add_constant(X_train[[most_correlated_feature]])).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run second model with additional, correlated features\n",
    "\n",
    "# Trial 1\n",
    "# select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'waterfront', 'view', 'grade']].copy()\n",
    "\n",
    "# Trial 2\n",
    "# select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'waterfront', 'view']].copy()\n",
    "\n",
    "# Trial 3\n",
    "# select_features = X_train[['sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'waterfront', 'view']].copy()\n",
    "\n",
    "# Trial 4\n",
    "# select_features = X_train[['sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'view']].copy()\n",
    "\n",
    "# Trial 5\n",
    "# select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'view', 'grade']].copy()\n",
    "\n",
    "\n",
    "# Trial 6\n",
    "# select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'sqft_above', 'sqft_basement']].copy()\n",
    "\n",
    "# Trial 7\n",
    "select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "                           'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "                           'sqft_above', 'sqft_basement']].copy()\n",
    "\n",
    "second_model = LinearRegression()\n",
    "\n",
    "second_model_scores = cross_validate(estimator=second_model,\n",
    "                                     X=select_features, y=y_train,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())\n",
    "\n",
    "# Trial 1:\n",
    "# Second Model\n",
    "# Train score:  0.496510401293593\n",
    "# Validation score:  0.49119845883778046\n",
    "# First Model\n",
    "# Train score:  0.4026275172522893\n",
    "# Validation score:  0.39002239307457764\n",
    "\n",
    "# Trial 2:\n",
    "# Second Model\n",
    "# Train score:  0.4288909091195216\n",
    "# Validation score:  0.430601865447946\n",
    "# First Model\n",
    "# Train score:  0.3824853370125143\n",
    "# Validation score:  0.37867372602330196\n",
    "\n",
    "# Trial 3:\n",
    "# Second Model\n",
    "# Train score:  0.42285711826946865\n",
    "# Validation score:  0.42393550511133987\n",
    "# First Model\n",
    "# Train score:  0.3824853370125143\n",
    "# Validation score:  0.37867372602330196\n",
    "\n",
    "# Trial 4:\n",
    "# Second Model\n",
    "# Train score:  0.42142246455384114\n",
    "# Validation score:  0.4216979102573514\n",
    "# First Model\n",
    "# Train score:  0.3824853370125143\n",
    "# Validation score:  0.37867372602330196\n",
    "\n",
    "# Trial 5:\n",
    "# Second Model\n",
    "# Train score:  0.4948170285278602\n",
    "# Validation score:  0.4885522178278873\n",
    "# First Model\n",
    "# Train score:  0.4026275172522893\n",
    "# Validation score:  0.39002239307457764\n",
    "\n",
    "# Trial 6:\n",
    "# Second Model\n",
    "# Train score:  0.40003825863753684\n",
    "# Validation score:  0.3941216058870845\n",
    "# First Model\n",
    "# Train score:  0.3824853370125143\n",
    "# Validation score:  0.37867372602330196\n",
    "\n",
    "# Trial 7:\n",
    "# Second Model\n",
    "# Train score:  0.5166094744927036\n",
    "# Validation score:  0.5121567236780332\n",
    "# First Model\n",
    "# Train score:  0.4026275172522893\n",
    "# Validation score:  0.39002239307457764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients of second model\n",
    "sm.OLS(y_train, sm.add_constant(select_features)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run third model with features with high p-value removed\n",
    "\n",
    "# Remove features due to high p-value and possible multicollinearity\n",
    "# Trials 1, 3, 4, 5\n",
    "# N/A\n",
    "\n",
    "# Trial #2\n",
    "# less_features = select_features.drop(columns=['bathrooms']).copy()\n",
    "\n",
    "# Trial #6\n",
    "# less_features = select_features.drop(columns=['bathrooms', 'sqft_above', 'sqft_basement']).copy()\n",
    "\n",
    "# Trial #7a\n",
    "# less_features = select_features.drop(columns=['floors', 'sqft_above', 'sqft_basement']).copy()\n",
    "\n",
    "# Trial #7b\n",
    "less_features = select_features.drop(columns=['floors', 'waterfront', 'sqft_above', 'sqft_basement']).copy()\n",
    "\n",
    "third_model = LinearRegression()\n",
    "\n",
    "third_model_scores = cross_validate(estimator=third_model,\n",
    "                                     X=less_features, y=y_train,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Third Model')\n",
    "print('Train score: ', third_model_scores['train_score'].mean())\n",
    "print('Validation score: ', third_model_scores['test_score'].mean())\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())\n",
    "\n",
    "# Trial 2:\n",
    "# Third Model\n",
    "# Train score:  0.42882665785284235\n",
    "# Validation score:  0.43085378605110547\n",
    "# Second Model\n",
    "# Train score:  0.4288909091195216\n",
    "# Validation score:  0.430601865447946\n",
    "# First Model\n",
    "# Train score:  0.3824853370125143\n",
    "# Validation score:  0.37867372602330196\n",
    "\n",
    "# Trial 6:\n",
    "# Third Model\n",
    "# Train score:  0.3966865317353403\n",
    "# Validation score:  0.39227606270023035\n",
    "# Second Model\n",
    "# Train score:  0.40003825863753684\n",
    "# Validation score:  0.3941216058870845\n",
    "# First Model\n",
    "# Train score:  0.3824853370125143\n",
    "# Validation score:  0.37867372602330196\n",
    "\n",
    "# Trial 7a:\n",
    "# Third Model\n",
    "# Train score:  0.5115716743687072\n",
    "# Validation score:  0.5087137858963889\n",
    "# Second Model\n",
    "# Train score:  0.5166094744927036\n",
    "# Validation score:  0.5121567236780332\n",
    "# First Model\n",
    "# Train score:  0.4026275172522893\n",
    "# Validation score:  0.39002239307457764\n",
    "\n",
    "# Trial 7b:\n",
    "# Third Model\n",
    "# Train score:  0.5097722382826495\n",
    "# Validation score:  0.5059020926945704\n",
    "# Second Model\n",
    "# Train score:  0.5166094744927036\n",
    "# Validation score:  0.5121567236780332\n",
    "# First Model\n",
    "# Train score:  0.4026275172522893\n",
    "# Validation score:  0.39002239307457764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients of third model\n",
    "# Trials 1, 3, 4, 5\n",
    "# N/A\n",
    "\n",
    "# Trials 2, 6, 7\n",
    "sm.OLS(y_train, sm.add_constant(less_features)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model and score it\n",
    "# Trial 1\n",
    "# final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'waterfront', 'view', 'grade']\n",
    "\n",
    "# Trial 2\n",
    "# final_features = ['bedrooms', 'sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'waterfront', 'view']\n",
    "\n",
    "# Trial 3\n",
    "# final_features = ['sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'waterfront', 'view']\n",
    "\n",
    "# Trial 4\n",
    "# final_features = ['sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'view']\n",
    "\n",
    "# Trial  5\n",
    "# final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'view', 'grade']\n",
    "\n",
    "# Trial  6\n",
    "# final_features = ['bedrooms', 'sqft_living', 'sqft_lot', 'floors']\n",
    "\n",
    "# Trial  7a\n",
    "# final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                   'waterfront', 'view', 'condition', 'grade']\n",
    "\n",
    "# Trial  7b\n",
    "final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "                  'view', 'condition', 'grade']\n",
    "\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]\n",
    "\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "final_model.score(X_test_final, y_test)\n",
    "\n",
    "# Trial 1 Score: 0.511874207507455\n",
    "# Trial 2 Score: 0.451456418439514\n",
    "# Trial 3 Score: 0.4438049237298961\n",
    "# Trial 4 Score: 0.44217810233401955\n",
    "# Trial 5 Score: 0.5092022681763618\n",
    "# Trial 6 Score: 0.41074053515426934\n",
    "# Trial 7a Score: 0.5290284607510969\n",
    "# Trial 7b Score: 0.5267250835532156"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "***\n",
    "Ca-ching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature correlation of training data\n",
    "\n",
    "# Trial 1\n",
    "# final_features_include_price = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                                 'floors', 'waterfront', 'view', 'grade']\n",
    "\n",
    "# Trial 2: \n",
    "# final_features_include_price = ['price', 'bedrooms', 'sqft_living', 'sqft_lot',\n",
    "#                                 'floors', 'waterfront', 'view']\n",
    "\n",
    "# Trial 3: \n",
    "# final_features_include_price = ['price', 'sqft_living', 'sqft_lot',\n",
    "#                                 'floors', 'waterfront', 'view']\n",
    "\n",
    "# Trial 4:\n",
    "# final_features_include_price = ['price', 'sqft_living', 'sqft_lot', 'floors', 'view']\n",
    "\n",
    "# Trial 5:\n",
    "# final_features_include_price = ['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "#                                 'sqft_lot', 'floors', 'view', 'grade']\n",
    "\n",
    "# Trial 6:\n",
    "# final_features_include_price = ['price', 'bedrooms', 'sqft_living', 'sqft_lot', 'floors']\n",
    "\n",
    "# Trial 7a:\n",
    "# final_features_include_price = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                                 'waterfront', 'view', 'condition', 'grade']\n",
    "\n",
    "# Trial 7b:\n",
    "final_features_include_price = ['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "                                'sqft_lot', 'view', 'condition', 'grade']\n",
    "\n",
    "\n",
    "final_features_infer_df = infer_df[final_features_include_price]\n",
    "corr = final_features_infer_df.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "            ax=ax,annot=True, cbar_kws={\"label\": \"Correlation\",\n",
    "                                        \"orientation\": \"horizontal\",\n",
    "                                        \"pad\": .2, \"extend\": \"both\"})\n",
    "ax.set_title('Correlation Heatmap of Inferential Model Features');\n",
    "plt.savefig('./data/correlation_heatmap.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RMSE\n",
    "RMSE = mean_squared_error(y_test, final_model.predict(X_test_final), squared=False)\n",
    "\n",
    "RMSE\n",
    "\n",
    "# Trial 1 RMSE: 173586.9619946568\n",
    "# Trial 2 RMSE: 184016.51147291518\n",
    "# Trial 3 RMSE: 185295.46668845217\n",
    "# Trial 4 RMSE: 185566.2552490975\n",
    "# Trial 5 RMSE: 174061.41023609342\n",
    "# Trial 6 RMSE: 190723.6316965016\n",
    "# Trial 7a RMSE: 170509.49027367876\n",
    "# Trial 7b RMSE: 170925.93650008747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients and intercept of final model\n",
    "print(pd.Series(final_model.coef_, index=X_train_final.columns, name=\"Coefficients\"))\n",
    "print(\"Intercept:\", final_model.intercept_)\n",
    "\n",
    "# Trial 1:\n",
    "# bedrooms       -12701.455316\n",
    "# bathrooms      -21503.827436\n",
    "# sqft_living       131.799022\n",
    "# sqft_lot           -1.004437\n",
    "# floors          -9873.331838\n",
    "# waterfront     201394.511585\n",
    "# view            40887.654188\n",
    "# grade           91304.929574\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: -355895.2075841872\n",
    "\n",
    "# Trial 2:\n",
    "# bedrooms       -28110.327140\n",
    "# sqft_living       209.106764\n",
    "# sqft_lot           -0.960157\n",
    "# floors          23539.481932\n",
    "# waterfront     180402.784943\n",
    "# view            45219.673734\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: 137305.31369272142\n",
    "\n",
    "# Trial 3:\n",
    "# sqft_living       187.609784\n",
    "# sqft_lot           -0.858892\n",
    "# floors          26848.382311\n",
    "# waterfront     184712.899065\n",
    "# view            47335.907533\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: 79592.55837849632\n",
    "\n",
    "# Trial 4:\n",
    "# sqft_living      186.707086\n",
    "# sqft_lot          -0.822644\n",
    "# floors         27252.036823\n",
    "# view           50456.581770\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: 80128.47086002585\n",
    "\n",
    "# Trial 5:\n",
    "# bedrooms      -12925.470940\n",
    "# bathrooms     -21570.658078\n",
    "# sqft_living      131.268752\n",
    "# sqft_lot          -0.965824\n",
    "# floors         -9301.170791\n",
    "# view           44287.779986\n",
    "# grade          91020.604718\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: -353361.70400261175\n",
    "\n",
    "# Trial 6:\n",
    "# bedrooms      -34895.027351\n",
    "# sqft_living      225.647839\n",
    "# sqft_lot          -0.906406\n",
    "# floors         17789.230753\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: 147838.76873069798\n",
    "\n",
    "# Trial 7a:\n",
    "# bedrooms       -15484.751485\n",
    "# bathrooms      -18483.736434\n",
    "# sqft_living       128.488448\n",
    "# sqft_lot           -1.037998\n",
    "# waterfront     207724.208474\n",
    "# view            39203.973986\n",
    "# condition       48150.886040\n",
    "# grade           95244.182246\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: -554302.4580548927\n",
    "\n",
    "# Trial 7b:\n",
    "# bedrooms      -15734.933848\n",
    "# bathrooms     -18384.720739\n",
    "# sqft_living      127.929927\n",
    "# sqft_lot          -1.000784\n",
    "# view           42698.987942\n",
    "# condition      47902.050447\n",
    "# grade          95015.067619\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: -550675.8977261952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig\n",
    "coef_s = pd.Series(final_model.coef_, index=X_train_final.columns, name=\"Coefficients\")\n",
    "coef_s.sort_values(ascending=False, inplace=True)\n",
    "colors_ax = ['darkgreen', 'darkgreen', 'darkgreen', 'darkgreen', 'darkred', 'darkred', 'darkred']\n",
    "display(coef_s)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.bar(coef_s.index, coef_s.values, color=colors_ax)\n",
    "ax.axhline(y=0, color='black')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Coefficients ($)')\n",
    "ax.set_title('Coefficients of Features')\n",
    "xlocs, xlabs = plt.xticks()\n",
    "for i, v in enumerate(coef_s):\n",
    "    string = ''\n",
    "    if v > 0:\n",
    "        string = '$' + str(abs(round(v,2)))\n",
    "        plt.text(xlocs[i] - 0.225, v + 1000, string)\n",
    "    else:\n",
    "        string = '-$' + str(abs(round(v,2)))\n",
    "        plt.text(xlocs[i] - 0.225, v - 3000, string)\n",
    "plt.savefig('./data/coefficients.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkinng independence (aka no multicollinearity) assumption holds\n",
    "vif = [variance_inflation_factor(X_train_final.values, i) for i in range(X_train_final.shape[1])]\n",
    "pd.Series(vif, index=X_train_final.columns, name=\"Variance Inflation Factor\")\n",
    "\n",
    "# Trial 1:\n",
    "# bedrooms       21.471617\n",
    "# bathrooms      24.188882\n",
    "# sqft_living    21.695238\n",
    "# sqft_lot        1.790673\n",
    "# floors         12.856621\n",
    "# waterfront      1.088276\n",
    "# view            1.232208\n",
    "# grade          32.032787\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 2: \n",
    "# bedrooms       14.669954\n",
    "# sqft_living    15.641708\n",
    "# sqft_lot        1.735450\n",
    "# floors          7.823340\n",
    "# waterfront      1.088188\n",
    "# view            1.226896\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 3: \n",
    "# sqft_living    8.207268\n",
    "# sqft_lot       1.726465\n",
    "# floors         6.828538\n",
    "# waterfront     1.088187\n",
    "# view           1.216567\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 4:\n",
    "# sqft_living    8.178242\n",
    "# sqft_lot       1.722206\n",
    "# floors         6.819759\n",
    "# view           1.123939\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 5:\n",
    "# bedrooms       21.471491\n",
    "# bathrooms      24.188339\n",
    "# sqft_living    21.665891\n",
    "# sqft_lot        1.786698\n",
    "# floors         12.849574\n",
    "# view            1.139868\n",
    "# grade          32.030612\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 6:\n",
    "# bedrooms       14.537254\n",
    "# sqft_living    14.949514\n",
    "# sqft_lot        1.730840\n",
    "# floors          7.793032\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 7a:\n",
    "# bedrooms       24.021589\n",
    "# bathrooms      21.743362\n",
    "# sqft_living    23.314119\n",
    "# sqft_lot        1.763216\n",
    "# waterfront      1.087689\n",
    "# view            1.225547\n",
    "# condition      20.083208\n",
    "# grade          44.842451\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 7b:\n",
    "# bedrooms       24.020809\n",
    "# bathrooms      21.743050\n",
    "# sqft_living    23.284192\n",
    "# sqft_lot        1.759836\n",
    "# view            1.134468\n",
    "# condition      20.083027\n",
    "# grade          44.833321\n",
    "# Name: Variance Inflation Factor, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_s = pd.Series(vif, index=X_train_final.columns, name=\"Variance Inflation Factor\")\n",
    "vif_s.sort_values(ascending=False, inplace=True)\n",
    "colors_ax = ['darkred', 'darkred', 'darkred', 'darkred', 'darkred', 'darkgreen', 'darkgreen']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(vif_s.index, vif_s.values, color=colors_ax)\n",
    "ax.axhline(y=5, color='black')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('VIF')\n",
    "ax.set_title('Variance Inflation Factors of Features');\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking linearity assumption holds\n",
    "preds = final_model.predict(X_test_final)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(y_test.min(), y_test.max())\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"red\", label=\"Perfect Fit\")\n",
    "ax.scatter(y_test, preds, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkinng normality assumption holds\n",
    "residuals = (y_test - preds)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking homoscedasticity\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(preds, residuals, alpha=0.5)\n",
    "ax.plot(preds, [0 for i in range(len(X_test))])\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model - Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model training and testing data\n",
    "# X = df.drop(columns=['id', 'date', 'yr_built', 'yr_renovated',\n",
    "#                      'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "# X = np.log(X+1)\n",
    "# y = X.price\n",
    "# X = X.drop(columns='price')\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine skew of final features and target\n",
    "# X_train.hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine target ('price') distribution\n",
    "# sns.distplot(y_train, fit=stats.norm)\n",
    "# fig = plt.figure()\n",
    "# stats.probplot(y_train, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show feature correlation of training data\n",
    "# train_data = pd.concat([X_train, y_train], axis=1)\n",
    "# corr = train_data.corr()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12,12))\n",
    "# sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "#             ax=ax,annot=True, cbar_kws={\"label\": \"Correlation\",\n",
    "#                                         \"orientation\": \"horizontal\",\n",
    "#                                         \"pad\": .2, \"extend\": \"both\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Show scatter plots of training data compared to target\n",
    "# fig, axes = plt.subplots(ncols=3, nrows=4, figsize=(16, 10))\n",
    "\n",
    "# fig.set_tight_layout(True)\n",
    "\n",
    "# for index, col in enumerate(X_train.columns):\n",
    "#     ax = axes[index//3][index%3]\n",
    "#     ax.scatter(X_train[col], y_train) #, alpha=0.2)\n",
    "#     ax.set_xlabel(col)\n",
    "#     ax.set_ylabel('price')\n",
    "\n",
    "# fig.delaxes(axes[3][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine target ('price') distribution\n",
    "# sns.distplot(y_train, fit=stats.norm)\n",
    "# fig = plt.figure()\n",
    "# stats.probplot(y_train, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run log function to normalize target data\n",
    "# y_train_log = np.log(y_train+1)\n",
    "# y_test_log = np.log(y_test+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-examine target ('price') \n",
    "# sns.distplot(y_train_log, fit=stats.norm)\n",
    "# fig = plt.figure()\n",
    "# stats.probplot(y_train_log, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine skew of final features\n",
    "# X_train.hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply log to continuous features and re-examine skew\n",
    "# X_train_continuous_log = pd.DataFrame([])\n",
    "# X_train_continuous_log['sqft_living_log'] = np.log(X_train['sqft_living']+1)\n",
    "# X_train_continuous_log['sqft_lot_log'] = np.log(X_train['sqft_lot']+1)\n",
    "# X_train_continuous_log['sqft_basement_log'] = np.log(X_train['sqft_basement']+1)\n",
    "# X_train_continuous_log.hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_discrete = X_train[['bedrooms', 'floors']]\n",
    "# train_data = pd.concat([X_train_continuous_log, X_train_discrete, y_train_log], axis=1)\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale all training features\n",
    "# train_columns = train_data.columns\n",
    "# scaler = StandardScaler()\n",
    "# train_scaled = scaler.fit_transform(train_data)\n",
    "# train_scaled_df = pd.DataFrame(train_scaled, columns=train_columns)\n",
    "# X_train_scaled = train_scaled_df.drop(columns=['price'])\n",
    "# y_train_scaled = train_scaled_df.price\n",
    "# display(X_train_scaled)\n",
    "# display(y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Repeat the above process for the test data\n",
    "# # X_test_continuous_log = pd.DataFrame([])\n",
    "# # X_test_continuous_log['sqft_living_log'] = np.log(X_test['sqft_living'])\n",
    "# # X_test_continuous_log['sqft_lot_log'] = np.log(X_test['sqft_lot'])\n",
    "\n",
    "# # X_test_discrete = X_test[['bedrooms', 'floors']]\n",
    "# # test_data = pd.concat([X_test_continuous_log, X_test_discrete, y_test_log], axis=1)\n",
    "# # display(test_data)\n",
    "\n",
    "# test_data = pd.concat([X_test, y_test], axis=1)\n",
    "# test_columns = test_data.columns\n",
    "# scaler = StandardScaler()\n",
    "# test_scaled = scaler.fit_transform(test_data)\n",
    "# test_scaled_df = pd.DataFrame(test_scaled, columns=test_columns)\n",
    "# X_test_scaled = test_scaled_df.drop(columns=['price'])\n",
    "# y_test_scaled = test_scaled_df.price\n",
    "# display(X_test_scaled)\n",
    "# display(y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final_features = X_train_scaled.columns\n",
    "\n",
    "# X_train_final = X_train_scaled[final_features]\n",
    "# X_test_final = X_test_scaled[final_features]\n",
    "\n",
    "# final_model = LinearRegression()\n",
    "# final_model.fit(X_train_final, y_train_scaled)\n",
    "\n",
    "# final_model.score(X_test_final, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.OLS(y_train_scaled, sm.add_constant(X_train_scaled)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check RMSE\n",
    "# RMSE = mean_squared_error(y_test_scaled, final_model.predict(X_test_final), squared=False)\n",
    "\n",
    "# RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Coefficients and intercept of final model\n",
    "# print(pd.Series(final_model.coef_, index=X_train_final.columns, name=\"Coefficients\"))\n",
    "# print(\"Intercept:\", final_model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking linearity assumption holds\n",
    "# preds = final_model.predict(X_test_final)\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# perfect_line = np.arange(y_test_scaled.min(), y_test_scaled.max())\n",
    "# ax.plot(perfect_line, linestyle=\"--\", color=\"red\", label=\"Perfect Fit\")\n",
    "# ax.scatter(y_test_scaled, preds, alpha=0.5)\n",
    "# ax.set_xlabel(\"Actual Price\")\n",
    "# ax.set_ylabel(\"Predicted Price\")\n",
    "# ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checkinng normality assumption holds\n",
    "# residuals = (y_test_scaled - preds)\n",
    "# sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checkinng independence (aka no multicollinearity) assumption holds\n",
    "# vif = [variance_inflation_factor(X_train_final.values, i) for i in range(X_train_final.shape[1])]\n",
    "# pd.Series(vif, index=X_train_final.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking homoscedasticity\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.scatter(preds, residuals, alpha=0.5)\n",
    "# ax.plot(preds, [0 for i in range(len(X_test_scaled))])\n",
    "# ax.set_xlabel(\"Predicted Value\")\n",
    "# ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # small sample because loading the whole dataframe will be too much to load\n",
    "# sample = df.sample(20,random_state=33)\n",
    "# # creates a map and centers its in the center of King County\n",
    "# mp = folium.Map(location=[sample.lat.mean(),\n",
    "#                           sample.long.mean()], zoom_start=10, control_scale=True)\n",
    "# # adds markers for the homes in the same and their price\n",
    "# for index, location_info in sample.iterrows():\n",
    "#     folium.Marker([location_info[\"lat\"], location_info[\"long\"]],\n",
    "#                   popup=\"$\" + str(location_info[\"price\"])).add_to(mp)\n",
    "# mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "***\n",
    "Here they are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusions\n",
    "***\n",
    "They are:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
