{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](./data/home-sales-shutterstock-295804091-1068x601.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Home Sales\n",
    "**Authors:** [Jerry Vasquez](https://www.linkedin.com/in/jerry-vasquez-832b71224/), [Paul Lindquist](https://www.linkedin.com/in/paul-lindquist/), [Vu Brown](https://www.linkedin.com/in/austin-brown-b5211384/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "***\n",
    "This is our overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "***\n",
    "This is our business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "***\n",
    "This is where the data is sourced from with focuses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "***\n",
    "Descriptive analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "***\n",
    "Notes on EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import time\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import seaborn as sns\n",
    "sns.set_theme(palette='magma_r')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 100) # Allows Jupyter Notebook to expand how much data is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with highest correlation to price\n",
    "price_corr = df.corr()['price'].map(abs).sort_values(ascending=False)\n",
    "price_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot top 6 feature correlations with price\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15,10))\n",
    "axs[0, 0].scatter(df.sqft_living, df.price)\n",
    "axs[0, 0].set_title('sqft_living')\n",
    "axs[0, 1].scatter(df.sqft_above, df.price)\n",
    "axs[0, 1].set_title('sqft_above')\n",
    "axs[1, 0].scatter(df.sqft_living15, df.price)\n",
    "axs[1, 0].set_title('sqft_living15')\n",
    "axs[1, 1].scatter(df.bathrooms, df.price)\n",
    "axs[1, 1].set_title('bathrooms')\n",
    "axs[2, 0].scatter(df.bedrooms, df.price)\n",
    "axs[2, 0].set_title('bedrooms')\n",
    "axs[2, 1].scatter(df.lat, df.price)\n",
    "axs[2, 1].set_title('lat')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(data=df, x='long', y='lat', hue='price', palette='magma_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning, Preparation, Feature Engineering, etc.\n",
    "***\n",
    "Identifying and dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to identify duplicates\n",
    "def determine_dupes(series):\n",
    "    series_vcs = pd.Series(series.value_counts())\n",
    "    series_dupes = [series_vcs.index[index] for index in range(len(series_vcs)) if series_vcs.values[index] > 1]\n",
    "    print(\"Amount of unique duplicates: \" + str(len(series_dupes)))\n",
    "    print(\"Total amount of duplicates: \" + str(series_vcs.values[0:len(series_dupes)].sum()))\n",
    "    \n",
    "    return series_vcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run duplicates function for 'id' series\n",
    "determine_dupes(df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df.loc[df.id == 795000620])\n",
    "# display(df[df.duplicated(subset=['id'], keep=False)].head(20))\n",
    "# display(df[df.duplicated(subset=['id'], keep='first')].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates found within 'id' series\n",
    "df = df.drop_duplicates(subset=['id'], keep='last')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[df.duplicated(subset=['lat','long'], keep=False)].sort_values('lat')\n",
    "# df = df.drop_duplicates(subset=['lat', 'long'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify and drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine number of bedrooms for outliers\n",
    "df.bedrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.bedrooms == 8].sort_values('sqft_living', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33 bedrooms for a 1620 sqft house is a mistake. We'll drop those values.\n",
    "# 9, 10 & 11 bedrooms for houses under 5000 sqft are also a mistake. We'll drop.\n",
    "df.drop(df.loc[df['bedrooms']==33].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==11].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==10].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==9].index, inplace=True)\n",
    "\n",
    "df.sort_values('bedrooms', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.bathrooms.value_counts()\n",
    "# df.loc[df.bathrooms == 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make features more workable by dealing with missing/bunk values and changing series from objects to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN/?/missing values with 0, None or No for respective series\n",
    "# Also change object series to integer via astype function\n",
    "df.yr_renovated = df.yr_renovated.fillna(0)\n",
    "df.yr_renovated = df.yr_renovated.astype('int64')\n",
    "\n",
    "df.view = df.view.fillna('NONE')\n",
    "\n",
    "df.waterfront = df.waterfront.fillna('NO')\n",
    "\n",
    "df.loc[df.sqft_basement == '?', 'sqft_basement'] = 0.0\n",
    "df.sqft_basement = df.sqft_basement.astype('float64').astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: sklearn - Change 'grade' series objects to integers\n",
    "# lb_make = LabelEncoder()\n",
    "# df['grade'] = lb_make.fit_transform(df['grade'])\n",
    "# df.grade.value_counts()\n",
    "# 8:Average, 9:Good, 10:Better, 7:Low Average, 0:Very Good\n",
    "# 1:Excellent, 6:Fair, 2:Luxury, 5:Low, 3:Mansion, 4:Poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# OPTION 2: pd.replace - Change 'grade' series objects to corresponding integers\n",
    "df.grade = pd.to_numeric(df.grade.map(lambda x: x.split()[0]))\n",
    "df['grade'].replace('3 Poor', 3, inplace=True)\n",
    "df['grade'].replace('4 Low', 4, inplace=True)\n",
    "df['grade'].replace('5 Fair', 5, inplace=True)\n",
    "df['grade'].replace('6 Low Average', 6, inplace=True)\n",
    "df['grade'].replace('7 Average', 7, inplace=True)\n",
    "df['grade'].replace('8 Good', 8, inplace=True)\n",
    "df['grade'].replace('9 Better', 9, inplace=True)\n",
    "df['grade'].replace('10 Very Good', 10, inplace=True)\n",
    "df['grade'].replace('11 Excellent', 11, inplace=True)\n",
    "df['grade'].replace('12 Luxury', 12, inplace=True)\n",
    "df['grade'].replace('13 Mansion', 13, inplace=True)\n",
    "df.grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: sklearn - Change 'condition' series objects to integers\n",
    "# lb_make = LabelEncoder()\n",
    "# df['condition'] = lb_make.fit_transform(df['condition'])\n",
    "# df.condition.value_counts()\n",
    "# 0:Average, 2:Good, 4:Very Good 1: Fair, 3:Poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTION 2: pd.replace - Change 'condition' series objects to corresponding integers\n",
    "# Integer values from https://info.kingcounty.gov/assessor/esales/Glossary.aspx\n",
    "df['condition'].replace('Poor', 1, inplace=True)\n",
    "df['condition'].replace('Fair', 2, inplace=True)\n",
    "df['condition'].replace('Average', 3, inplace=True)\n",
    "df['condition'].replace('Good', 4, inplace=True)\n",
    "df['condition'].replace('Very Good', 5, inplace=True)\n",
    "df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTION 1: sklearn - Change 'waterfront' series objects to integers\n",
    "# lb_make = LabelEncoder()\n",
    "# df['waterfront'] = lb_make.fit_transform(df['waterfront'])\n",
    "# df.waterfront.value_counts()\n",
    "# 0:NO, 1:YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: pd.cat.codes - Change 'waterfront' series objects to category\n",
    "# df.waterfront = df.waterfront.astype('category')\n",
    "# df.waterfront.cat.codes\n",
    "# df.waterfront.value_counts()\n",
    "# 0:NO, 1:YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 3: pd.replace - Change 'waterfront' series YES/NO objects to corresponding 0/1 integers \n",
    "df['waterfront'].replace('NO', 0, inplace=True)\n",
    "df['waterfront'].replace('YES', 1, inplace=True)\n",
    "df.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: sklearn - Change 'view' series objects to integers\n",
    "# lb_make = LabelEncoder()\n",
    "# df['view'] = lb_make.fit_transform(df['view'])\n",
    "# df.view.value_counts()\n",
    "# 4:NONE, 0:AVERAGE, 3:GOOD, 2:FAIR, 1:EXCELLENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTION 2: pd.replace - Change 'view' series objects to corresponding integers\n",
    "# Integer values mirrored from 'condition' series\n",
    "df['view'].replace('NONE', 0, inplace=True)\n",
    "df['view'].replace('FAIR', 2, inplace=True)\n",
    "df['view'].replace('AVERAGE', 3, inplace=True)\n",
    "df['view'].replace('GOOD', 4, inplace=True)\n",
    "df['view'].replace('EXCELLENT', 5, inplace=True)\n",
    "df.view.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Change 'date' series to datetime data type (may not be needed)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling\n",
    "***\n",
    "In this section, we create a predictive model using many correlated features. We'll normalize and scale all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model training and testing data\n",
    "X = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target ('price') distribution\n",
    "sns.distplot(y_train, fit=stats.norm)\n",
    "fig = plt.figure()\n",
    "stats.probplot(y_train, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run log function to normalize target data\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-examine target ('price') \n",
    "sns.distplot(y_train_log, fit=stats.norm)\n",
    "fig = plt.figure()\n",
    "stats.probplot(y_train_log, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature correlation of training data\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "corr = train_data.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "            ax=ax,annot=True, cbar_kws={\"label\": \"Correlation\",\n",
    "                                        \"orientation\": \"horizontal\",\n",
    "                                        \"pad\": .2, \"extend\": \"both\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show linear correlation with 'price' & 'sqft_living'\n",
    "most_correlated_feature = 'sqft_living'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train[most_correlated_feature], y_train, alpha=0.5)\n",
    "ax.set_xlabel('sqft_living')\n",
    "ax.set_ylabel('price')\n",
    "ax.set_title('Most Correlated Feature vs. Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model with DummyRegressor\n",
    "baseline = DummyRegressor()\n",
    "baseline.fit(X_train, y_train_log)\n",
    "baseline.score(X_test, y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline model with highested correlated feature ('sqft_living')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "first_model = LinearRegression()\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    "\n",
    "first_scores = cross_validate(estimator=first_model,\n",
    "                                 X=X_train[[most_correlated_feature]],\n",
    "                                 y=y_train_log, return_train_score=True,\n",
    "                                 cv=splitter)\n",
    "\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional, correlated features to X_train data\n",
    "select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "                             'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "                             'sqft_above', 'sqft_basement', 'sqft_living15',\n",
    "                             'sqft_lot15']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2nd model with additional, correlated features\n",
    "second_model_with_ylog = LinearRegression()\n",
    "\n",
    "second_model_scores = cross_validate(estimator=second_model_with_ylog,\n",
    "                                     X=select_features, y=y_train_log,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients\n",
    "sm.OLS(y_train_log, sm.add_constant(select_features)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'sqft_basement' due to high p-value and possible multicollinearity\n",
    "less_features = select_features.drop(['sqft_basement'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run 3rd model with 'sqft_basement' removed\n",
    "third_model_with_ylog = LinearRegression()\n",
    "\n",
    "third_model_scores = cross_validate(estimator=third_model_with_ylog,\n",
    "                                     X=less_features, y=y_train_log,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Third Model')\n",
    "print('Train score: ', third_model_scores['train_score'].mean())\n",
    "print('Validation score: ', third_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use recursive feature elimination and feature selection to examine significant features\n",
    "X_train_for_RFECV = StandardScaler().fit_transform(less_features)\n",
    "\n",
    "model_for_RFECV = LinearRegression()\n",
    "\n",
    "selector = RFECV(model_for_RFECV, cv=splitter)\n",
    "selector.fit(X_train_for_RFECV, y_train_log)\n",
    "\n",
    "print(\"Was the column selected?\")\n",
    "for index, col in enumerate(less_features.columns):\n",
    "    print(f\"{col}: {selector.support_[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a final model with the settled-on, selected features. This is also where we'll normalize (log) and scale the remaining data (independent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "                  'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "                  'sqft_living15', 'sqft_lot15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model and score it\n",
    "\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]\n",
    "\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_final, y_train_log)\n",
    "\n",
    "final_model.score(X_test_final, y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RMSE\n",
    "mean_squared_error(y_test_log, final_model.predict(X_test_final), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to log and scale independent variables (X_train, X_test) and scale target variable (y_train_log, y_test_log). Note, target already had log applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Examine skew of final features\n",
    "X_train[final_features].hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log to continuous features and re-examine skew\n",
    "X_train_continuous_log = pd.DataFrame([])\n",
    "X_train_continuous_log['sqft_living_log'] = np.log(X_train['sqft_living'])\n",
    "X_train_continuous_log['sqft_lot_log'] = np.log(X_train['sqft_lot'])\n",
    "X_train_continuous_log['sqft_above_log'] = np.log(X_train['sqft_above'])\n",
    "X_train_continuous_log['sqft_living15_log'] = np.log(X_train['sqft_living15'])\n",
    "X_train_continuous_log['sqft_lot15_log'] = np.log(X_train['sqft_lot15'])\n",
    "X_train_continuous_log.hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of all train features (independent & target) so\n",
    "# everything can be scaled\n",
    "X_train_discreet = X_train[['bedrooms', 'bathrooms', 'floors', 'waterfront',\n",
    "                           'view', 'condition', 'grade']]\n",
    "\n",
    "X_train_cont_disc = pd.concat([X_train_continuous_log, X_train_discreet, y_train_log],\n",
    "                              axis=1)\n",
    "\n",
    "train_columns = X_train_cont_disc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scale all training features\n",
    "scaler = StandardScaler()\n",
    "X_train_log_scaled = scaler.fit_transform(X_train_cont_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-separate target and independent features\n",
    "X_train_full = pd.DataFrame(X_train_log_scaled, columns=train_columns)\n",
    "\n",
    "y_train_log_scaled = X_train_full['price']\n",
    "X_train_log_scaled = X_train_full.drop(columns=['price'])\n",
    "X_train_log_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above process for the testing data\n",
    "X_test_continuous_log = pd.DataFrame([])\n",
    "X_test_continuous_log['sqft_living_log'] = np.log(X_test['sqft_living'])\n",
    "X_test_continuous_log['sqft_lot_log'] = np.log(X_test['sqft_lot'])\n",
    "X_test_continuous_log['sqft_above_log'] = np.log(X_test['sqft_above'])\n",
    "X_test_continuous_log['sqft_living15_log'] = np.log(X_test['sqft_living15'])\n",
    "X_test_continuous_log['sqft_lot15_log'] = np.log(X_test['sqft_lot15'])\n",
    "\n",
    "X_test_discreet = X_test[['bedrooms', 'bathrooms', 'floors', 'waterfront',\n",
    "                          'view', 'condition', 'grade']]\n",
    "X_test_cont_disc = pd.concat([X_test_continuous_log, X_test_discreet, y_test_log],\n",
    "                              axis=1)\n",
    "test_columns = X_test_cont_disc.columns\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "X_test_log_scaled = scaler2.fit_transform(X_test_cont_disc)\n",
    "\n",
    "X_test_full = pd.DataFrame(X_test_log_scaled, columns=test_columns)\n",
    "\n",
    "y_test_log_scaled = X_test_full['price']\n",
    "X_test_log_scaled = X_test_full.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, run and score final model using log and scaled data\n",
    "final_model_log_scaled = LinearRegression()\n",
    "final_model_log_scaled.fit(X_train_log_scaled, y_train_log_scaled)\n",
    "\n",
    "final_model_log_scaled.score(X_test_log_scaled, y_test_log_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find scaled RMSE\n",
    "RMSE_log_scaled = mean_squared_error(y_test_log_scaled,\n",
    "                   final_model_log_scaled.predict(X_test_log_scaled),\n",
    "                   squared=False)\n",
    "np.exp(RMSE_log_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_model_log_scaled.predict(X_test_log_scaled)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(y_test_log_scaled.min(), y_test_log_scaled.max())\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"orange\", label=\"Perfect Fit\")\n",
    "ax.scatter(y_test_log_scaled, preds, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y_test_log_scaled - preds)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity (Independence Assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = [variance_inflation_factor(X_train_log_scaled.values, i) for i in range(X_train_log_scaled.shape[1])]\n",
    "pd.Series(vif, index=X_train_log_scaled.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(preds, residuals, alpha=0.5)\n",
    "ax.plot(preds, [0 for i in range(len(X_test_log_scaled))])\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "***\n",
    "Ca-ching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "***\n",
    "Here they are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusions\n",
    "***\n",
    "They are:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
