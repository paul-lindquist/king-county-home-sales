{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](./data/home-sales-shutterstock-295804091-1068x601.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Home Sales\n",
    "**Authors:** [Jerry Vasquez](https://www.linkedin.com/in/jerry-vasquez-832b71224/), [Paul Lindquist](https://www.linkedin.com/in/paul-lindquist/), [Vu Brown](https://www.linkedin.com/in/austin-brown-b5211384/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "***\n",
    "This is our overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "***\n",
    "This is our business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "***\n",
    "This is where the data is sourced from with focuses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "***\n",
    "Descriptive analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from collections import Counter\n",
    "import folium\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "sns.set_theme(palette='magma_r')\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder, MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 500) # Allows Jupyter Notebook to expand how much data is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Exploratory Data Analysis\n",
    "Understanding the aspects of the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with highest correlation to price\n",
    "price_corr = df.corr()['price'].map(abs).sort_values(ascending=False)\n",
    "price_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latitude and longitude coordinates to examine where home prices are\n",
    "# the highest\n",
    "df_minus_outliers = df[df.price < (df.price.mean() + 3*df.price.std())].copy()\n",
    "\n",
    "fig = px.scatter_mapbox(df_minus_outliers, lat=\"lat\", lon=\"long\", color=\"price\",\n",
    "#                         color_discrete_sequence=[\"IceFire\"], zoom=10.2, height=1000)\n",
    "                        color_discrete_sequence=[\"IceFire\"], zoom=10.5, height=1000)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to identify duplicates\n",
    "def determine_dupes(series):\n",
    "    series_vcs = pd.Series(series.value_counts())\n",
    "    series_dupes = [series_vcs.index[index] for index in range(len(series_vcs)) if series_vcs.values[index] > 1]\n",
    "    print(\"Amount of unique duplicates: \" + str(len(series_dupes)))\n",
    "    print(\"Total amount of duplicates: \" + str(series_vcs.values[0:len(series_dupes)].sum()))\n",
    "    \n",
    "    return series_vcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run duplicates function for 'id' series\n",
    "determine_dupes(df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop duplicates found within 'id' series\n",
    "df = df.drop_duplicates(subset=['id'], keep='last')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Consider droping duplicates based upon latitude and longitude\n",
    "# df[df.duplicated(subset=['lat','long'], keep=False)].sort_values('lat')\n",
    "# df = df.drop_duplicates(subset=['lat', 'long'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making features useful for regression modeling by dealing with missing/bunk values and changing series from objects to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN/?/missing values with 0, None or No for respective series\n",
    "# Also change object series to integer via astype function\n",
    "df.yr_renovated = df.yr_renovated.fillna(0)\n",
    "df.yr_renovated = df.yr_renovated.astype('int64')\n",
    "\n",
    "df.view = df.view.fillna('NONE')\n",
    "\n",
    "df.waterfront = df.waterfront.fillna('NO')\n",
    "\n",
    "df.loc[df.sqft_basement == '?', 'sqft_basement'] = 0.0\n",
    "df.sqft_basement = df.sqft_basement.astype('float64').astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving issues with `grade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'grade' series objects to corresponding integers\n",
    "df.grade = pd.to_numeric(df.grade.map(lambda x: x.split()[0]))\n",
    "df.grade.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving issues with `condition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'condition' series objects to corresponding integers\n",
    "# Integer values from https://info.kingcounty.gov/assessor/esales/Glossary.aspx\n",
    "df['condition'].replace('Poor', 1, inplace=True)\n",
    "df['condition'].replace('Fair', 2, inplace=True)\n",
    "df['condition'].replace('Average', 3, inplace=True)\n",
    "df['condition'].replace('Good', 4, inplace=True)\n",
    "df['condition'].replace('Very Good', 5, inplace=True)\n",
    "df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving issues with `waterfront`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'waterfront' series objects to integers\n",
    "lb_make = LabelEncoder()\n",
    "df['waterfront'] = lb_make.fit_transform(df['waterfront'])\n",
    "df.waterfront.value_counts()\n",
    "# 0:NO, 1:YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving issues with `view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.view.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'view' series objects to corresponding integers\n",
    "# Integer values mirrored from 'condition' series\n",
    "df['view'].replace('NONE', 0, inplace=True)\n",
    "df['view'].replace('FAIR', 2, inplace=True)\n",
    "df['view'].replace('AVERAGE', 3, inplace=True)\n",
    "df['view'].replace('GOOD', 4, inplace=True)\n",
    "df['view'].replace('EXCELLENT', 5, inplace=True)\n",
    "df.view.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving issues with `date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'date' series to datetime data type (may not be needed)\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify & Drop Outliers for Inferential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inferential dataframe\n",
    "infer_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to help with removal of outliers\n",
    "def determine_outliers_cut_off(series, constant):\n",
    "    price_outliers_low = series.mean() - constant*series.std()\n",
    "    price_outliers_high = series.mean() + constant*series.std()\n",
    "    return price_outliers_high, price_outliers_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine price\n",
    "infer_df.price.hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine price outliers by calculating at least 3 std. dev.'s from the mean\n",
    "price_outliers_high, price_outliers_low = determine_outliers_cut_off(infer_df.price, 3)\n",
    "print(price_outliers_low)\n",
    "print(price_outliers_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine bedrooms\n",
    "display(infer_df.bedrooms.value_counts())\n",
    "\n",
    "bedrooms_outliers_high, bedrooms_outliers_low = determine_outliers_cut_off(infer_df.bedrooms, 3)\n",
    "print(bedrooms_outliers_low)\n",
    "print(bedrooms_outliers_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine bathrooms\n",
    "display(infer_df.bathrooms.value_counts())\n",
    "\n",
    "bathrooms_outliers_high, bathrooms_outliers_low = determine_outliers_cut_off(infer_df.bathrooms, 3)\n",
    "print(bathrooms_outliers_low)\n",
    "print(bathrooms_outliers_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine floors\n",
    "display(infer_df.floors.value_counts())\n",
    "\n",
    "floors_outliers_high, floors_outliers_low = determine_outliers_cut_off(infer_df.floors, 3)\n",
    "print(floors_outliers_low)\n",
    "print(floors_outliers_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine sqft_living\n",
    "constant = 3.5\n",
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_living, infer_df.price)\n",
    "axs.axvline(infer_df.sqft_living.mean() + constant*infer_df.sqft_living.std())\n",
    "axs.set_title('sqft_living');\n",
    "\n",
    "sqft_living_outliers_high, sqft_living_outliers_low = determine_outliers_cut_off(infer_df.sqft_living, constant)\n",
    "print(sqft_living_outliers_low)\n",
    "print(sqft_living_outliers_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sqft_lot\n",
    "constant = 3\n",
    "fig, axs = plt.subplots(figsize=(20,10))\n",
    "axs.scatter(infer_df.sqft_lot, infer_df.price)\n",
    "axs.axvline(infer_df.sqft_lot.mean() + constant*infer_df.sqft_lot.std())\n",
    "axs.set_title('sqft_lot');\n",
    "\n",
    "sqft_lot_outliers_high, sqft_lot_outliers_low = determine_outliers_cut_off(infer_df.sqft_lot, constant)\n",
    "print(sqft_lot_outliers_low)\n",
    "print(sqft_lot_outliers_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sqft_above\n",
    "constant = 3\n",
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_above, infer_df.price)\n",
    "axs.axvline(infer_df.sqft_above.mean() + constant*infer_df.sqft_above.std())\n",
    "axs.set_title('sqft_above');\n",
    "\n",
    "sqft_above_outliers_high, sqft_above_outliers_low = determine_outliers_cut_off(infer_df.sqft_above, constant)\n",
    "print(sqft_above_outliers_low)\n",
    "print(sqft_above_outliers_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sqft_basement\n",
    "constant = 3\n",
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_basement, infer_df.price)\n",
    "axs.axvline(infer_df.sqft_basement.mean() + constant*infer_df.sqft_basement.std())\n",
    "axs.set_title('sqft_basement');\n",
    "\n",
    "sqft_basement_outliers_high, sqft_basement_outliers_low = determine_outliers_cut_off(infer_df.sqft_basement, constant)\n",
    "print(sqft_basement_outliers_low)\n",
    "print(sqft_basement_outliers_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.price < price_outliers_high]\n",
    "display(infer_df.info())\n",
    "infer_df.price.hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.bedrooms < bedrooms_outliers_high]\n",
    "display(infer_df.info())\n",
    "infer_df.bedrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.bathrooms < bathrooms_outliers_high]\n",
    "display(infer_df.info())\n",
    "infer_df.bathrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.floors < floors_outliers_high]\n",
    "display(infer_df.info())\n",
    "infer_df.floors.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.sqft_living < sqft_living_outliers_high]\n",
    "display(infer_df.info())\n",
    "\n",
    "constant = 3.5\n",
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_living, infer_df.price)\n",
    "axs.set_title('sqft_living');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.sqft_lot < sqft_lot_outliers_high]\n",
    "display(infer_df.info())\n",
    "\n",
    "constant = 3\n",
    "fig, axs = plt.subplots(figsize=(20,10))\n",
    "axs.scatter(infer_df.sqft_lot, infer_df.price)\n",
    "axs.set_title('sqft_lot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.sqft_above < sqft_above_outliers_high]\n",
    "display(infer_df.info())\n",
    "\n",
    "constant = 3\n",
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_above, infer_df.price)\n",
    "axs.set_title('sqft_above');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "infer_df = infer_df[infer_df.sqft_basement < sqft_basement_outliers_high]\n",
    "display(infer_df.info())\n",
    "\n",
    "constant = 3\n",
    "fig, axs = plt.subplots(figsize=(10,4))\n",
    "axs.scatter(infer_df.sqft_basement, infer_df.price)\n",
    "axs.set_title('sqft_basement');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify & Drop Outliers for Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.bedrooms == 33].sort_values('sqft_living', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33 bedrooms for a 1620 sqft house is a mistake. We'll drop those values.\n",
    "# 9, 10 & 11 bedrooms for houses under 5000 sqft are also a mistake. We'll drop.\n",
    "df.drop(df.loc[df['bedrooms']==33].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==11].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==10].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==9].index, inplace=True)\n",
    "\n",
    "df.sort_values('bedrooms', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Modeling\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows the iterative approach taken towards finding the best inferential model by first determining relevant features of interest, and then analyzing the coefficients of determination, coefficients of features, p-values, and other statistically relevent aspects of the models. In addition, the findings and results of each iteration/trial is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model training and testing data\n",
    "\n",
    "# Trial 1\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'condition', 'sqft_above',\n",
    "#                            'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "#                            'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 2 (Difference from Trial 1--> Dropped grade)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'condition', 'grade', 'sqft_above',\n",
    "#                            'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', \n",
    "#                            'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 3 (Difference from Trial 2--> Dropped bedrooms and bathrooms)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'bedrooms', 'bathrooms', 'condition',\n",
    "#                            'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',\n",
    "#                            'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 4 (Difference from Trial 3--> Dropped waterfront)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'bedrooms', 'bathrooms', 'waterfront',\n",
    "#                            'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built',\n",
    "#                            'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 5 (Difference from Trial 1--> Dropped waterfront)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'waterfront', 'condition', 'sqft_above',\n",
    "#                            'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n",
    "#                            'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 6 (Difference from Trial 1--> Dropped waterfront, view, grade; Added sqft_above, sqft_basement)\n",
    "# X = infer_df.drop(columns=['price', 'id', 'date', 'waterfront', 'view', 'condition', 'grade',\n",
    "#                            'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "# Trial 7 (Difference from Trial 1--> Added condition, sqft_above, sqft_basement)\n",
    "X = infer_df.drop(columns=['price', 'id', 'date', 'yr_built', 'yr_renovated',\n",
    "                           'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
    "\n",
    "y = infer_df.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show feature correlation of training data\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "corr = train_data.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "            ax=ax,annot=True, cbar_kws={\"label\": \"Correlation\",\n",
    "                                        \"orientation\": \"horizontal\",\n",
    "                                        \"pad\": .2, \"extend\": \"both\"});\n",
    "\n",
    "# Trial 1 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "# sqft_living & grade\n",
    "\n",
    "# Trial 2 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "\n",
    "# Trials 3, 4 - Multicollinearity Concerns:\n",
    "# sqft_living & price\n",
    "\n",
    "# Trial 5 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "# sqft_living & grade\n",
    "\n",
    "# Trial 6 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "# sqft_living & sqft_above\n",
    "\n",
    "# Trial 7 - Multicollinearity Concerns:\n",
    "# sqft_living & bathrooms\n",
    "# sqft_living & grade\n",
    "# sqft_living & sqft_above\n",
    "# sqft_above & grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show scatter plots of training data compared to target\n",
    "\n",
    "# Trials 1, 2, 5, 6\n",
    "# fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(16, 10))\n",
    "\n",
    "# Trials 3, 4\n",
    "# fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(16, 7))\n",
    "\n",
    "# Trial 7 \n",
    "fig, axes = plt.subplots(ncols=3, nrows=4, figsize=(16, 10))\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "for index, col in enumerate(X_train.columns):\n",
    "    ax = axes[index//3][index%3]\n",
    "    ax.scatter(X_train[col], y_train) #, alpha=0.2)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('price')\n",
    "\n",
    "    \n",
    "# Trial 1\n",
    "# fig.delaxes(axes[2][2])\n",
    "\n",
    "# Trials 2, 5, 6\n",
    "# fig.delaxes(axes[2][1])\n",
    "# fig.delaxes(axes[2][2])\n",
    "\n",
    "# Trial 3\n",
    "# fig.delaxes(axes[1][2])\n",
    "\n",
    "# Trial 4\n",
    "# fig.delaxes(axes[1][1])\n",
    "# fig.delaxes(axes[1][2])\n",
    "\n",
    "# Trial 7\n",
    "fig.delaxes(axes[3][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model with DummyRegressor\n",
    "baseline = DummyRegressor()\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first model with highested correlated feature ('sqft_living')\n",
    "\n",
    "# Trials 1, 5, 7\n",
    "most_correlated_feature = 'grade'\n",
    "\n",
    "# Trial 2, 3, 4, 6\n",
    "# most_correlated_feature = 'sqft_living'\n",
    "\n",
    "first_model = LinearRegression()\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    "\n",
    "first_scores = cross_validate(estimator=first_model,\n",
    "                                 X=X_train[[most_correlated_feature]],\n",
    "                                 y=y_train, return_train_score=True,\n",
    "                                 cv=splitter)\n",
    "\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())\n",
    "\n",
    "# Trials 1, 5, 7:\n",
    "# First Model\n",
    "# Train score:  0.414588498037074\n",
    "# Validation score:  0.42492697175493815\n",
    "\n",
    "# Trials 2, 3, 4, 6:\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients of first model\n",
    "sm.OLS(y_train, sm.add_constant(X_train[[most_correlated_feature]])).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run second model with additional, correlated features\n",
    "\n",
    "# Trial 1\n",
    "# select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'waterfront', 'view', 'grade']].copy()\n",
    "\n",
    "# Trial 2\n",
    "# select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'waterfront', 'view']].copy()\n",
    "\n",
    "# Trial 3\n",
    "# select_features = X_train[['sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'waterfront', 'view']].copy()\n",
    "\n",
    "# Trial 4\n",
    "# select_features = X_train[['sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'view']].copy()\n",
    "\n",
    "# Trial 5\n",
    "# select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'view', 'grade']].copy()\n",
    "\n",
    "\n",
    "# Trial 6\n",
    "# select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                            'floors', 'sqft_above', 'sqft_basement']].copy()\n",
    "\n",
    "# Trial 7\n",
    "select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "                           'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "                           'sqft_above', 'sqft_basement']].copy()\n",
    "\n",
    "second_model = LinearRegression()\n",
    "\n",
    "second_model_scores = cross_validate(estimator=second_model,\n",
    "                                     X=select_features, y=y_train,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())\n",
    "\n",
    "# Trial 1:\n",
    "# Second Model\n",
    "# Train score:  0.5115381808835865\n",
    "# Validation score:  0.5185176054579146\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 2:\n",
    "# Second Model\n",
    "# Train score:  0.4472809084845386\n",
    "# Validation score:  0.4499643386092405\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 3:\n",
    "# Second Model\n",
    "# Train score:  0.4405937590300398\n",
    "# Validation score:  0.44493075864886356\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 4:\n",
    "# Second Model\n",
    "# Train score:  0.43840158886968944\n",
    "# Validation score:  0.4432119138653097\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 5:\n",
    "# Second Model\n",
    "# Train score:  0.5087900596439866\n",
    "# Validation score:  0.5161810311821672\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 6:\n",
    "# Second Model\n",
    "# Train score:  0.4151475204412618\n",
    "# Validation score:  0.42144705522457837\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 7:\n",
    "# Second Model\n",
    "# Train score:  0.5322909097073351\n",
    "# Validation score:  0.5360463814225995\n",
    "# First Model\n",
    "# Train score:  0.414588498037074\n",
    "# Validation score:  0.42492697175493815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients of second model\n",
    "sm.OLS(y_train, sm.add_constant(select_features)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run third model with features with high p-value removed\n",
    "\n",
    "# Remove features due to high p-value and possible multicollinearity\n",
    "# Trials 1, 3, 4, 5\n",
    "# N/A\n",
    "\n",
    "# Trial #2\n",
    "# less_features = select_features.drop(columns=['bathrooms']).copy()\n",
    "\n",
    "# Trial #6\n",
    "# less_features = select_features.drop(columns=['bathrooms', 'sqft_above', 'sqft_basement']).copy()\n",
    "\n",
    "# Trial #7a\n",
    "# less_features = select_features.drop(columns=['floors', 'sqft_above', 'sqft_basement']).copy()\n",
    "\n",
    "# Trial #7b\n",
    "less_features = select_features.drop(columns=['floors', 'waterfront', 'sqft_above', 'sqft_basement']).copy()\n",
    "\n",
    "third_model = LinearRegression()\n",
    "\n",
    "third_model_scores = cross_validate(estimator=third_model,\n",
    "                                     X=less_features, y=y_train,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Third Model')\n",
    "print('Train score: ', third_model_scores['train_score'].mean())\n",
    "print('Validation score: ', third_model_scores['test_score'].mean())\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())\n",
    "\n",
    "# Trial 2:\n",
    "# Third Model\n",
    "# Train score:  0.4472589810970505\n",
    "# Validation score:  0.4499575049891514\n",
    "# Second Model\n",
    "# Train score:  0.4472809084845386\n",
    "# Validation score:  0.4499643386092405\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 6:\n",
    "# Third Model\n",
    "# Train score:  0.4120920473349818\n",
    "# Validation score:  0.41827420141563426\n",
    "# Second Model\n",
    "# Train score:  0.4151475204412618\n",
    "# Validation score:  0.42144705522457837\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 7a:\n",
    "# Third Model\n",
    "# Train score:  0.5277460988907076\n",
    "# Validation score:  0.5323939628237843\n",
    "# Second Model\n",
    "# Train score:  0.5322909097073351\n",
    "# Validation score:  0.5360463814225995\n",
    "# First Model\n",
    "# Train score:  0.3976366000836915\n",
    "# Validation score:  0.40635354544352903\n",
    "\n",
    "# Trial 7b:\n",
    "# Third Model\n",
    "# Train score:  0.5249436553249848\n",
    "# Validation score:  0.5300064733067893\n",
    "# Second Model\n",
    "# Train score:  0.5322909097073351\n",
    "# Validation score:  0.5360463814225995\n",
    "# First Model\n",
    "# Train score:  0.414588498037074\n",
    "# Validation score:  0.42492697175493815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients of third model\n",
    "# Trials 1, 3, 4, 5\n",
    "# N/A\n",
    "\n",
    "# Trials 2, 6, 7\n",
    "sm.OLS(y_train, sm.add_constant(less_features)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model and score it\n",
    "# Trial 1\n",
    "# final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'waterfront', 'view', 'grade']\n",
    "\n",
    "# Trial 2\n",
    "# final_features = ['bedrooms', 'sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'waterfront', 'view']\n",
    "\n",
    "# Trial 3\n",
    "# final_features = ['sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'waterfront', 'view']\n",
    "\n",
    "# Trial 4\n",
    "# final_features = ['sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'view']\n",
    "\n",
    "# Trial 5\n",
    "# final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                   'floors', 'view', 'grade']\n",
    "\n",
    "# Trial 6\n",
    "# final_features = ['bedrooms', 'sqft_living', 'sqft_lot', 'floors']\n",
    "\n",
    "# Trial 7a\n",
    "# final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                   'waterfront', 'view', 'condition', 'grade']\n",
    "\n",
    "# Trial 7b\n",
    "final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "                  'view', 'condition', 'grade']\n",
    "\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]\n",
    "\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "final_model.score(X_test_final, y_test)\n",
    "\n",
    "# Trial 1 Score: 0.5122565930691938\n",
    "# Trial 2 Score: 0.45218493940429394\n",
    "# Trial 3 Score: 0.4449265646174718\n",
    "# Trial 4 Score: 0.4449057795826621\n",
    "# Trial 5 Score: 0.5120769199763275\n",
    "# Trial 6 Score: 0.4203671368154732\n",
    "# Trial 7a Score: 0.527617569659234\n",
    "# Trial 7b Score: 0.527653369069877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "***\n",
    "This section provides the RMSE, coefficients of features, intercept, and the four assumptions of linear regression for the final inferential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature correlation of training data\n",
    "\n",
    "# Trial 1\n",
    "# final_features_include_price = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                                 'floors', 'waterfront', 'view', 'grade']\n",
    "\n",
    "# Trial 2: \n",
    "# final_features_include_price = ['price', 'bedrooms', 'sqft_living', 'sqft_lot',\n",
    "#                                 'floors', 'waterfront', 'view']\n",
    "\n",
    "# Trial 3: \n",
    "# final_features_include_price = ['price', 'sqft_living', 'sqft_lot',\n",
    "#                                 'floors', 'waterfront', 'view']\n",
    "\n",
    "# Trial 4:\n",
    "# final_features_include_price = ['price', 'sqft_living', 'sqft_lot', 'floors', 'view']\n",
    "\n",
    "# Trial 5:\n",
    "# final_features_include_price = ['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "#                                 'sqft_lot', 'floors', 'view', 'grade']\n",
    "\n",
    "# Trial 6:\n",
    "# final_features_include_price = ['price', 'bedrooms', 'sqft_living', 'sqft_lot', 'floors']\n",
    "\n",
    "# Trial 7a:\n",
    "# final_features_include_price = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "#                                 'waterfront', 'view', 'condition', 'grade']\n",
    "\n",
    "# Trial 7b:\n",
    "final_features_include_price = ['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "                                'sqft_lot', 'view', 'condition', 'grade']\n",
    "\n",
    "\n",
    "final_features_infer_df = infer_df[final_features_include_price]\n",
    "corr = final_features_infer_df.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "            ax=ax,annot=True, cbar_kws={\"label\": \"Correlation\",\n",
    "                                        \"orientation\": \"horizontal\",\n",
    "                                        \"pad\": .2, \"extend\": \"both\"})\n",
    "ax.set_title('Correlation Heatmap of Inferential Model Features');\n",
    "plt.savefig('./data/correlation_heatmap.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RMSE\n",
    "RMSE = mean_squared_error(y_test, final_model.predict(X_test_final), squared=False)\n",
    "\n",
    "RMSE\n",
    "\n",
    "# Trial 1 RMSE: 171891.21890108107\n",
    "# Trial 2 RMSE: 182169.2084828494\n",
    "# Trial 3 RMSE: 183372.0791291402\n",
    "# Trial 4 RMSE: 183375.5123319613\n",
    "# Trial 5 RMSE: 171922.8763082007\n",
    "# Trial 6 RMSE: 187384.85530621544\n",
    "# Trial 7a RMSE: 169162.79631882257\n",
    "# Trial 7b RMSE: 169156.38621255787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Coefficients and intercept of final model\n",
    "print(pd.Series(final_model.coef_, index=X_train_final.columns, name=\"Coefficients\"))\n",
    "print(\"Intercept:\", final_model.intercept_)\n",
    "\n",
    "# Trial 1:\n",
    "# bedrooms       -12403.223041\n",
    "# bathrooms      -24266.956596\n",
    "# sqft_living       134.121810\n",
    "# sqft_lot           -1.088879\n",
    "# floors         -10654.493719\n",
    "# waterfront     229531.849627\n",
    "# view            40836.318305\n",
    "# grade           93085.076678\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: -366070.7543283864\n",
    "\n",
    "# Trial 2:\n",
    "# bedrooms       -29063.830949\n",
    "# sqft_living       213.256807\n",
    "# sqft_lot           -1.030757\n",
    "# floors          22355.715531\n",
    "# waterfront     194527.027628\n",
    "# view            45831.812383\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: 136144.5077016906\n",
    "\n",
    "# Trial 3:\n",
    "# sqft_living       191.904577\n",
    "# sqft_lot           -0.919129\n",
    "# floors          25587.279592\n",
    "# waterfront     202699.625657\n",
    "# view            47821.575812\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: 75236.04639607953\n",
    "\n",
    "# Trial 4:\n",
    "# sqft_living      190.853129\n",
    "# sqft_lot          -0.870734\n",
    "# floors         26097.979106\n",
    "# view           51603.310306\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: 75743.4946812251\n",
    "\n",
    "# Trial 5:\n",
    "# bedrooms      -12935.899346\n",
    "# bathrooms     -24223.335353\n",
    "# sqft_living      133.802466\n",
    "# sqft_lot          -1.035701\n",
    "# floors         -9878.714194\n",
    "# view           45113.873883\n",
    "# grade          92505.116943\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: -361427.00863892934\n",
    "\n",
    "# Trial 6:\n",
    "# bedrooms      -35927.312809\n",
    "# sqft_living      229.127926\n",
    "# sqft_lot          -0.988000\n",
    "# floors         15505.418021\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: 150018.0302155278\n",
    "\n",
    "# Trial 7a:\n",
    "# bedrooms       -15310.733187\n",
    "# bathrooms      -21054.230584\n",
    "# sqft_living       130.654034\n",
    "# sqft_lot           -1.114860\n",
    "# waterfront     231748.926119\n",
    "# view            39120.879396\n",
    "# condition       49844.313685\n",
    "# grade           97399.866558\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: -573847.2957910688\n",
    "\n",
    "# Trial 7b:\n",
    "# bedrooms      -15882.999485\n",
    "# bathrooms     -20761.202482\n",
    "# sqft_living      130.298547\n",
    "# sqft_lot          -1.064769\n",
    "# view           43412.287164\n",
    "# condition      49681.336403\n",
    "# grade          96913.876757\n",
    "# Name: Coefficients, dtype: float64\n",
    "# Intercept: -568477.6204495409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 7b\n",
    "coef_s = pd.Series(final_model.coef_, index=X_train_final.columns, name=\"Coefficients\")\n",
    "coef_s.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "sns.set_style(style='dark')\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax = sns.barplot(coef_s.index, coef_s.values, palette='magma_r')#, color=colors_ax)\n",
    "ax.axhline(y=0, color='black')\n",
    "ax.set_xlabel('Features', fontsize=14, labelpad=13)\n",
    "ax.set_ylabel('Coefficients ($)', fontsize=14)\n",
    "ax.set_title('Coefficients of Features', fontsize=15)\n",
    "ax.ticklabel_format(axis='y', useOffset=False, style='plain')\n",
    "y = np.array([-20000, 0, 20000, 40000, 60000, 80000, 100000])\n",
    "y_ticks_labels = [\"-20K\",\"0\", \"20K\", \"40K\", \"60K\", \"80K\", \"100K\"]\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(y_ticks_labels)\n",
    "xlocs, xlabs = plt.xticks()\n",
    "for i, v in enumerate(coef_s):\n",
    "    string = ''\n",
    "    if v > 0:\n",
    "        string = '$' + str(abs(round(v,2)))\n",
    "        plt.text(xlocs[i] - 0.225, v + 1000, string, weight='bold')\n",
    "    else:\n",
    "        string = '-$' + str(abs(round(v,2)))\n",
    "        plt.text(xlocs[i] - 0.225, v - 3000, string, weight='bold')\n",
    "plt.savefig('./data/coefficients.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkinng independence (aka no multicollinearity) assumption holds\n",
    "vif = [variance_inflation_factor(X_train_final.values, i) for i in range(X_train_final.shape[1])]\n",
    "pd.Series(vif, index=X_train_final.columns, name=\"Variance Inflation Factor\")\n",
    "\n",
    "# Trial 1:\n",
    "# bedrooms       21.353913\n",
    "# bathrooms      24.684748\n",
    "# sqft_living    21.009740\n",
    "# sqft_lot        1.756314\n",
    "# floors         13.073733\n",
    "# waterfront      1.098957\n",
    "# view            1.239649\n",
    "# grade          32.465846\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 2: \n",
    "# bedrooms       14.387326\n",
    "# sqft_living    14.998293\n",
    "# sqft_lot        1.705328\n",
    "# floors          7.919926\n",
    "# waterfront      1.098914\n",
    "# view            1.231901\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 3: \n",
    "# sqft_living    8.113156\n",
    "# sqft_lot       1.698304\n",
    "# floors         6.783588\n",
    "# waterfront     1.098832\n",
    "# view           1.225531\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 4:\n",
    "# sqft_living    8.080519\n",
    "# sqft_lot       1.692461\n",
    "# floors         6.774367\n",
    "# view           1.122524\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 5:\n",
    "# bedrooms       21.351724\n",
    "# bathrooms      24.684676\n",
    "# sqft_living    20.983958\n",
    "# sqft_lot        1.750573\n",
    "# floors         13.064506\n",
    "# view            1.137428\n",
    "# grade          32.464611\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 6:\n",
    "# bedrooms       14.299366\n",
    "# sqft_living    14.393143\n",
    "# sqft_lot        1.699181\n",
    "# floors          7.882150\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 7a:\n",
    "# bedrooms       24.192977\n",
    "# bathrooms      22.115402\n",
    "# sqft_living    22.793305\n",
    "# sqft_lot        1.728665\n",
    "# waterfront      1.098486\n",
    "# view            1.229774\n",
    "# condition      20.103667\n",
    "# grade          45.568216\n",
    "# Name: Variance Inflation Factor, dtype: float64\n",
    "\n",
    "# Trial 7b:\n",
    "# bedrooms       24.185220\n",
    "# bathrooms      22.113130\n",
    "# sqft_living    22.772153\n",
    "# sqft_lot        1.723913\n",
    "# view            1.129638\n",
    "# condition      20.098086\n",
    "# grade          45.567261\n",
    "# Name: Variance Inflation Factor, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking linearity assumption holds\n",
    "preds = final_model.predict(X_test_final)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(y_test.min(), y_test.max())\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"red\", label=\"Perfect Fit\")\n",
    "ax.scatter(y_test, preds, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkinng normality assumption holds\n",
    "residuals = (y_test - preds)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking homoscedasticity assumption holds\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(preds, residuals, alpha=0.5)\n",
    "ax.plot(preds, [0 for i in range(len(X_test))])\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling\n",
    "***\n",
    "In this section, we take an iterative approach to create a predictive model using many correlated features. We'll normalize and scale all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model training and testing data\n",
    "X = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target ('price') distribution\n",
    "sns.distplot(y_train, fit=stats.norm)\n",
    "fig = plt.figure()\n",
    "stats.probplot(y_train, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run log function to normalize target data\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-examine target ('price') \n",
    "sns.distplot(y_train_log, fit=stats.norm)\n",
    "fig = plt.figure()\n",
    "stats.probplot(y_train_log, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature correlation of training data\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "corr = train_data.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "            ax=ax,annot=True, cbar_kws={\"label\": \"Correlation\",\n",
    "                                        \"orientation\": \"horizontal\",\n",
    "                                        \"pad\": .2, \"extend\": \"both\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show linear correlation with 'price' & 'sqft_living'\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(df_minus_outliers.sqft_living, df_minus_outliers.price, alpha=0.5)\n",
    "ax.set_xlabel('Living Area in Sq Ft')\n",
    "ax.set_ylabel('Price')\n",
    "ax.ticklabel_format(style='plain', axis='y')\n",
    "ax.set_title('Living Area (Ft) vs. Price', fontsize=13)\n",
    "m, b = np.polyfit(df_minus_outliers.sqft_living, df_minus_outliers.price, 1)\n",
    "plt.plot(df_minus_outliers.sqft_living, m*df_minus_outliers.sqft_living + b, color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model with DummyRegressor\n",
    "baseline = DummyRegressor()\n",
    "baseline.fit(X_train, y_train_log)\n",
    "baseline.score(X_test, y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline model with highested correlated feature ('sqft_living')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "first_model = LinearRegression()\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    "\n",
    "first_scores = cross_validate(estimator=first_model,\n",
    "                                 X=X_train[['sqft_living']],\n",
    "                                 y=y_train_log, return_train_score=True,\n",
    "                                 cv=splitter)\n",
    "\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional, correlated features to X_train data\n",
    "select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "                             'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "                             'sqft_above', 'sqft_basement', 'sqft_living15',\n",
    "                             'sqft_lot15']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2nd model with additional, correlated features\n",
    "second_model_with_ylog = LinearRegression()\n",
    "\n",
    "second_model_scores = cross_validate(estimator=second_model_with_ylog,\n",
    "                                     X=select_features, y=y_train_log,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients\n",
    "sm.OLS(y_train_log, sm.add_constant(select_features)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'sqft_basement' due to high p-value and possible multicollinearity\n",
    "less_features = select_features.drop(['sqft_basement'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run 3rd model with 'sqft_basement' removed\n",
    "third_model_with_ylog = LinearRegression()\n",
    "\n",
    "third_model_scores = cross_validate(estimator=third_model_with_ylog,\n",
    "                                     X=less_features, y=y_train_log,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Third Model')\n",
    "print('Train score: ', third_model_scores['train_score'].mean())\n",
    "print('Validation score: ', third_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use recursive feature elimination and feature selection to examine significant features\n",
    "X_train_for_RFECV = StandardScaler().fit_transform(less_features)\n",
    "\n",
    "model_for_RFECV = LinearRegression()\n",
    "\n",
    "selector = RFECV(model_for_RFECV, cv=splitter)\n",
    "selector.fit(X_train_for_RFECV, y_train_log)\n",
    "\n",
    "print(\"Was the column selected?\")\n",
    "for index, col in enumerate(less_features.columns):\n",
    "    print(f\"{col}: {selector.support_[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a final model with the settled-on, selected features. This is also where we'll normalize (log) and scale the remaining data (independent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "                  'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "                  'sqft_living15', 'sqft_lot15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model and score it\n",
    "\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]\n",
    "\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_final, y_train_log)\n",
    "\n",
    "final_model.score(X_test_final, y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RMSE\n",
    "mean_squared_error(y_test_log, final_model.predict(X_test_final), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to log and scale independent variables (X_train, X_test) and scale target variable (y_train_log, y_test_log). Note, target already had log applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine skew of final features\n",
    "X_train[final_features].hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log to continuous features and re-examine skew\n",
    "X_train_continuous_log = pd.DataFrame([])\n",
    "X_train_continuous_log['sqft_living_log'] = np.log(X_train['sqft_living'])\n",
    "X_train_continuous_log['sqft_lot_log'] = np.log(X_train['sqft_lot'])\n",
    "X_train_continuous_log['sqft_above_log'] = np.log(X_train['sqft_above'])\n",
    "X_train_continuous_log['sqft_living15_log'] = np.log(X_train['sqft_living15'])\n",
    "X_train_continuous_log['sqft_lot15_log'] = np.log(X_train['sqft_lot15'])\n",
    "X_train_continuous_log.hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of all train features (independent & target) so\n",
    "# everything can be scaled\n",
    "X_train_discreet = X_train[['bedrooms', 'bathrooms', 'floors', 'waterfront',\n",
    "                           'view', 'condition', 'grade']]\n",
    "\n",
    "X_train_cont_disc = pd.concat([X_train_continuous_log, X_train_discreet, y_train_log],\n",
    "                              axis=1)\n",
    "\n",
    "train_columns = X_train_cont_disc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all training features\n",
    "scaler = StandardScaler()\n",
    "X_train_log_scaled = scaler.fit_transform(X_train_cont_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-separate target and independent features\n",
    "X_train_full = pd.DataFrame(X_train_log_scaled, columns=train_columns)\n",
    "\n",
    "y_train_log_scaled = X_train_full['price']\n",
    "X_train_log_scaled = X_train_full.drop(columns=['price'])\n",
    "X_train_log_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above process for the testing data\n",
    "X_test_continuous_log = pd.DataFrame([])\n",
    "X_test_continuous_log['sqft_living_log'] = np.log(X_test['sqft_living'])\n",
    "X_test_continuous_log['sqft_lot_log'] = np.log(X_test['sqft_lot'])\n",
    "X_test_continuous_log['sqft_above_log'] = np.log(X_test['sqft_above'])\n",
    "X_test_continuous_log['sqft_living15_log'] = np.log(X_test['sqft_living15'])\n",
    "X_test_continuous_log['sqft_lot15_log'] = np.log(X_test['sqft_lot15'])\n",
    "\n",
    "X_test_discreet = X_test[['bedrooms', 'bathrooms', 'floors', 'waterfront',\n",
    "                          'view', 'condition', 'grade']]\n",
    "X_test_cont_disc = pd.concat([X_test_continuous_log, X_test_discreet, y_test_log],\n",
    "                              axis=1)\n",
    "test_columns = X_test_cont_disc.columns\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "X_test_log_scaled = scaler2.fit_transform(X_test_cont_disc)\n",
    "\n",
    "X_test_full = pd.DataFrame(X_test_log_scaled, columns=test_columns)\n",
    "\n",
    "y_test_log_scaled = X_test_full['price']\n",
    "X_test_log_scaled = X_test_full.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, run and score final model using log and scaled data\n",
    "final_model_log_scaled = LinearRegression()\n",
    "final_model_log_scaled.fit(X_train_log_scaled, y_train_log_scaled)\n",
    "\n",
    "final_model_log_scaled.score(X_test_log_scaled, y_test_log_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find normalized-scaled RMSE\n",
    "RMSE_log_scaled = mean_squared_error(y_test_log_scaled,\n",
    "                   final_model_log_scaled.predict(X_test_log_scaled),\n",
    "                   squared=False)\n",
    "RMSE_log_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert normalized-scaled RMSE back to USD\n",
    "target_log = pd.concat([y_train_log_scaled, y_test_log_scaled], axis=0)\n",
    "\n",
    "y_hat_train = final_model_log_scaled.predict(X_train_log_scaled)\n",
    "y_hat_test = final_model_log_scaled.predict(X_test_log_scaled)\n",
    "\n",
    "def inv_normalize_price(feature_normalized):\n",
    "\n",
    "    mu = target_log.mean()\n",
    "    sd = target_log.std()\n",
    "    return sd*feature_normalized + mu\n",
    "\n",
    "inv1 = 10**(inv_normalize_price(y_train_log_scaled))\n",
    "inv2 = 10**(inv_normalize_price(y_hat_train))\n",
    "inv3 = 10**(inv_normalize_price(y_test_log_scaled))\n",
    "inv4 = 10**(inv_normalize_price(y_hat_test))\n",
    "\n",
    "# Transform back to regular $USD price (not log price)\n",
    "train_mse_non_log = mean_squared_error(inv1, inv2)\n",
    "test_mse_non_log = mean_squared_error(inv3, inv4)\n",
    "\n",
    "# Take the square root of MSE to find RMSE * 100 for USD units\n",
    "non_log_train = round(np.sqrt(train_mse_non_log)*100, 2)\n",
    "non_log_test = round(np.sqrt(test_mse_non_log)*100, 2)\n",
    "\n",
    "print(f'Train RMSE non-log: ${non_log_train}')\n",
    "print(f'Test RMSE non-log: ${non_log_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Linear Assumptions (though not as important for predictive purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_model_log_scaled.predict(X_test_log_scaled)\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "pred_xticks = ['0', '0', '.4M', '.8M', '1.2M', '1.6M']\n",
    "pred_yticks = ['0', '-.75M', '-.5M', '-.25M', '0', '.25M', '.5M', '1M', '1.25M', '1.5M']\n",
    "perfect_line = np.arange(y_test_log_scaled.min(), y_test_log_scaled.max(), step=.9)\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"black\", label=\"Perfect Fit\")\n",
    "ax.scatter(y_test_log_scaled+4.1, preds+.4, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.set_xticklabels(pred_xticks)\n",
    "ax.set_yticklabels(pred_yticks)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y_test_log_scaled - preds)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity (Independence Assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = [variance_inflation_factor(X_train_log_scaled.values, i) for i in range(X_train_log_scaled.shape[1])]\n",
    "pd.Series(vif, index=X_train_log_scaled.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(preds, residuals, alpha=0.5)\n",
    "ax.plot(preds, [0 for i in range(len(X_test_log_scaled))])\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "***\n",
    "Given the provided dataset and linear regression approach, both our inferential and predictive models did not perform as well as we had hoped.\n",
    "- For the inferential model, homoscedasticity was poor, multicollinearity (VIF) was too high and the RMSE range was north of \\$173k+. Linearity and normality performed decently.\n",
    "- For the predictive model, multicollinearity (VIF) was too high for several features, the model score was an underwhelming .601629 and the RMSE range of \\$193k+ was too great to provide any predictive value. Linearity and homoscedasticity were decent and normality was excellent, due to normalizing the data in pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "***\n",
    "We recommend not using our models for inferential or predictive purposes and perhaps looking into different modeling approaches other than linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusions\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
