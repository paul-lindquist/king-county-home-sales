{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import time\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import seaborn as sns\n",
    "sns.set_theme(palette='magma_r')\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 100) # Allows Jupyter Notebook to expand how much data is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_corr = df.corr()['price'].map(abs).sort_values(ascending=False)\n",
    "price_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 6 feature correlations with price\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15,10))\n",
    "axs[0, 0].scatter(df.sqft_living, df.price)\n",
    "axs[0, 0].set_title('sqft_living')\n",
    "axs[0, 1].scatter(df.sqft_above, df.price)\n",
    "axs[0, 1].set_title('sqft_above')\n",
    "axs[1, 0].scatter(df.sqft_living15, df.price)\n",
    "axs[1, 0].set_title('sqft_living15')\n",
    "axs[1, 1].scatter(df.bathrooms, df.price)\n",
    "axs[1, 1].set_title('bathrooms')\n",
    "axs[2, 0].scatter(df.bedrooms, df.price)\n",
    "axs[2, 0].set_title('bedrooms')\n",
    "axs[2, 1].scatter(df.lat, df.price)\n",
    "axs[2, 1].set_title('lat')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(data=df, x='long', y='lat', hue='price', palette='magma_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Preparation, Feature Engineering, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to identify duplicates\n",
    "def determine_dupes(series):\n",
    "    series_vcs = pd.Series(series.value_counts())\n",
    "    series_dupes = [series_vcs.index[index] for index in range(len(series_vcs)) if series_vcs.values[index] > 1]\n",
    "    print(\"Amount of unique duplicates: \" + str(len(series_dupes)))\n",
    "    print(\"Total amount of duplicates: \" + str(series_vcs.values[0:len(series_dupes)].sum()))\n",
    "    \n",
    "    return series_vcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run duplicates function for 'id' series\n",
    "determine_dupes(df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates found within 'id' series\n",
    "df = df.drop_duplicates(subset=['id'], keep='last')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine number of bedrooms for outliers\n",
    "df.bedrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33 bedrooms for a 1620 sqft house is a mistake. We'll drop those values.\n",
    "# 9, 10 & 11 bedrooms for houses under 5000 sqft are also a mistake. We'll drop.\n",
    "df.drop(df.loc[df['bedrooms']==33].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==11].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==10].index, inplace=True)\n",
    "df.drop(df.loc[df['bedrooms']==9].index, inplace=True)\n",
    "\n",
    "df.sort_values('bedrooms', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN/?/missing values with 0, None or No for respective series\n",
    "# Also change object series to integer via astype function\n",
    "df.yr_renovated = df.yr_renovated.fillna(0)\n",
    "df.yr_renovated = df.yr_renovated.astype('int64')\n",
    "\n",
    "df.view = df.view.fillna('NONE')\n",
    "\n",
    "df.waterfront = df.waterfront.fillna('NO')\n",
    "\n",
    "df.loc[df.sqft_basement == '?', 'sqft_basement'] = 0.0\n",
    "df.sqft_basement = df.sqft_basement.astype('float64').astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.grade = pd.to_numeric(df.grade.map(lambda x: x.split()[0]))\n",
    "df['grade'].replace('3 Poor', 3, inplace=True)\n",
    "df['grade'].replace('4 Low', 4, inplace=True)\n",
    "df['grade'].replace('5 Fair', 5, inplace=True)\n",
    "df['grade'].replace('6 Low Average', 6, inplace=True)\n",
    "df['grade'].replace('7 Average', 7, inplace=True)\n",
    "df['grade'].replace('8 Good', 8, inplace=True)\n",
    "df['grade'].replace('9 Better', 9, inplace=True)\n",
    "df['grade'].replace('10 Very Good', 10, inplace=True)\n",
    "df['grade'].replace('11 Excellent', 11, inplace=True)\n",
    "df['grade'].replace('12 Luxury', 12, inplace=True)\n",
    "df['grade'].replace('13 Mansion', 13, inplace=True)\n",
    "df.grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['condition'].replace('Poor', 1, inplace=True)\n",
    "df['condition'].replace('Fair', 2, inplace=True)\n",
    "df['condition'].replace('Average', 3, inplace=True)\n",
    "df['condition'].replace('Good', 4, inplace=True)\n",
    "df['condition'].replace('Very Good', 5, inplace=True)\n",
    "df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['waterfront'].replace('NO', 0, inplace=True)\n",
    "df['waterfront'].replace('YES', 1, inplace=True)\n",
    "df.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['view'].replace('NONE', 0, inplace=True)\n",
    "df['view'].replace('FAIR', 2, inplace=True)\n",
    "df['view'].replace('AVERAGE', 3, inplace=True)\n",
    "df['view'].replace('GOOD', 4, inplace=True)\n",
    "df['view'].replace('EXCELLENT', 5, inplace=True)\n",
    "df.view.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'date' series to datetime data type (may not be needed)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(15,10))\n",
    "axs[0, 0].scatter(df.sqft_living, df.price)\n",
    "axs[0, 0].set_title('Living Area (Ft²) vs. Price', fontsize=13)\n",
    "axs[0, 0].set_xlabel('Living Area in Sq Ft')\n",
    "axs[0, 0].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "axs[0, 1].scatter(df.bedrooms, df.price)\n",
    "axs[0, 1].set_title('# of Bedrooms vs. Price', fontsize=13)\n",
    "axs[0, 1].set_xlabel('Bedrooms')\n",
    "axs[0, 1].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "axs[1, 0].scatter(df.sqft_basement, df.price)\n",
    "axs[1, 0].set_title('Basement Size (Ft²) vs. Price', fontsize=13)\n",
    "axs[1, 0].set_xlabel('Basement Size in Sq Ft')\n",
    "axs[1, 0].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "axs[1, 1].scatter(df.sqft_lot, df.price)\n",
    "axs[1, 1].set_title('Lot Size (Acres) vs. Price', fontsize=13)\n",
    "axs[1, 1].set_xlabel('Lot Size in Acres')\n",
    "axs[1, 1].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "axs[2, 0].scatter(df.floors, df.price)\n",
    "axs[2, 0].set_title('# of Floors vs. Price', fontsize=13)\n",
    "axs[2, 0].set_xlabel('Floors')\n",
    "axs[2, 0].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(ylabel='Price')\n",
    "    \n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling\n",
    "In this section, we take an iterative approach to create a predictive model using many correlated features. We'll normalize and scale all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model training and testing data\n",
    "X = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target ('price') distribution\n",
    "sns.distplot(y_train, fit=stats.norm)\n",
    "fig = plt.figure()\n",
    "stats.probplot(y_train, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run log function to normalize target data\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-examine target ('price') \n",
    "sns.distplot(y_train_log, fit=stats.norm)\n",
    "fig = plt.figure()\n",
    "stats.probplot(y_train_log, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature correlation of training data\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "corr = train_data.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "            ax=ax,annot=True, cbar_kws={\"label\": \"Correlation\",\n",
    "                                        \"orientation\": \"horizontal\",\n",
    "                                        \"pad\": .2, \"extend\": \"both\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show linear correlation with 'price' & 'sqft_living'\n",
    "most_correlated_feature = 'sqft_living'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train[most_correlated_feature], y_train, alpha=0.5)\n",
    "ax.set_xlabel('sqft_living')\n",
    "ax.set_ylabel('price')\n",
    "ax.set_title('Most Correlated Feature vs. Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model with DummyRegressor\n",
    "baseline = DummyRegressor()\n",
    "baseline.fit(X_train, y_train_log)\n",
    "baseline.score(X_test, y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline model with highested correlated feature ('sqft_living')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "first_model = LinearRegression()\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    "\n",
    "first_scores = cross_validate(estimator=first_model,\n",
    "                                 X=X_train[[most_correlated_feature]],\n",
    "                                 y=y_train_log, return_train_score=True,\n",
    "                                 cv=splitter)\n",
    "\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional, correlated features to X_train data\n",
    "select_features = X_train[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "                             'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "                             'sqft_above', 'sqft_basement', 'sqft_living15',\n",
    "                             'sqft_lot15']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2nd model with additional, correlated features\n",
    "second_model_with_ylog = LinearRegression()\n",
    "\n",
    "second_model_scores = cross_validate(estimator=second_model_with_ylog,\n",
    "                                     X=select_features, y=y_train_log,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine OLS summary table to examine coefficients\n",
    "sm.OLS(y_train_log, sm.add_constant(select_features)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'sqft_basement' due to high p-value and possible multicollinearity\n",
    "less_features = select_features.drop(['sqft_basement'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run 3rd model with 'sqft_basement' removed\n",
    "third_model_with_ylog = LinearRegression()\n",
    "\n",
    "third_model_scores = cross_validate(estimator=third_model_with_ylog,\n",
    "                                     X=less_features, y=y_train_log,\n",
    "                                     return_train_score=True, cv=splitter)\n",
    "\n",
    "print('Third Model')\n",
    "print('Train score: ', third_model_scores['train_score'].mean())\n",
    "print('Validation score: ', third_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('Second Model')\n",
    "print('Train score: ', second_model_scores['train_score'].mean())\n",
    "print('Validation score: ', second_model_scores['test_score'].mean())\n",
    "print()\n",
    "print('First Model')\n",
    "print('Train score: ', first_scores['train_score'].mean())\n",
    "print('Validation score: ', first_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use recursive feature elimination and feature selection to examine significant features\n",
    "X_train_for_RFECV = StandardScaler().fit_transform(less_features)\n",
    "\n",
    "model_for_RFECV = LinearRegression()\n",
    "\n",
    "selector = RFECV(model_for_RFECV, cv=splitter)\n",
    "selector.fit(X_train_for_RFECV, y_train_log)\n",
    "\n",
    "print(\"Was the column selected?\")\n",
    "for index, col in enumerate(less_features.columns):\n",
    "    print(f\"{col}: {selector.support_[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a final model with the settled-on, selected features. This is also where we'll normalize (log) and scale the remaining data (independent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "                  'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "                  'sqft_living15', 'sqft_lot15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model and score it\n",
    "\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]\n",
    "\n",
    "final_model = LinearRegression()\n",
    "final_model.fit(X_train_final, y_train_log)\n",
    "\n",
    "final_model.score(X_test_final, y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RMSE\n",
    "mean_squared_error(y_test_log, final_model.predict(X_test_final), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to log and scale independent variables (X_train, X_test) and scale target variable (y_train_log, y_test_log). Note, target already had log applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine skew of final features\n",
    "X_train[final_features].hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log to continuous features and re-examine skew\n",
    "X_train_continuous_log = pd.DataFrame([])\n",
    "X_train_continuous_log['sqft_living_log'] = np.log(X_train['sqft_living'])\n",
    "X_train_continuous_log['sqft_lot_log'] = np.log(X_train['sqft_lot'])\n",
    "X_train_continuous_log['sqft_above_log'] = np.log(X_train['sqft_above'])\n",
    "X_train_continuous_log['sqft_living15_log'] = np.log(X_train['sqft_living15'])\n",
    "X_train_continuous_log['sqft_lot15_log'] = np.log(X_train['sqft_lot15'])\n",
    "X_train_continuous_log.hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of all train features (independent & target) so\n",
    "# everything can be scaled\n",
    "X_train_discreet = X_train[['bedrooms', 'bathrooms', 'floors', 'waterfront',\n",
    "                           'view', 'condition', 'grade']]\n",
    "\n",
    "X_train_cont_disc = pd.concat([X_train_continuous_log, X_train_discreet, y_train_log],\n",
    "                              axis=1)\n",
    "\n",
    "train_columns = X_train_cont_disc.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all training features\n",
    "scaler = StandardScaler()\n",
    "X_train_log_scaled = scaler.fit_transform(X_train_cont_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-separate target and independent features\n",
    "X_train_full = pd.DataFrame(X_train_log_scaled, columns=train_columns)\n",
    "\n",
    "y_train_log_scaled = X_train_full['price']\n",
    "X_train_log_scaled = X_train_full.drop(columns=['price'])\n",
    "X_train_log_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above process for the testing data\n",
    "X_test_continuous_log = pd.DataFrame([])\n",
    "X_test_continuous_log['sqft_living_log'] = np.log(X_test['sqft_living'])\n",
    "X_test_continuous_log['sqft_lot_log'] = np.log(X_test['sqft_lot'])\n",
    "X_test_continuous_log['sqft_above_log'] = np.log(X_test['sqft_above'])\n",
    "X_test_continuous_log['sqft_living15_log'] = np.log(X_test['sqft_living15'])\n",
    "X_test_continuous_log['sqft_lot15_log'] = np.log(X_test['sqft_lot15'])\n",
    "\n",
    "X_test_discreet = X_test[['bedrooms', 'bathrooms', 'floors', 'waterfront',\n",
    "                          'view', 'condition', 'grade']]\n",
    "X_test_cont_disc = pd.concat([X_test_continuous_log, X_test_discreet, y_test_log],\n",
    "                              axis=1)\n",
    "test_columns = X_test_cont_disc.columns\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "X_test_log_scaled = scaler2.fit_transform(X_test_cont_disc)\n",
    "\n",
    "X_test_full = pd.DataFrame(X_test_log_scaled, columns=test_columns)\n",
    "\n",
    "y_test_log_scaled = X_test_full['price']\n",
    "X_test_log_scaled = X_test_full.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, run and score final model using log and scaled data\n",
    "final_model_log_scaled = LinearRegression()\n",
    "final_model_log_scaled.fit(X_train_log_scaled, y_train_log_scaled)\n",
    "\n",
    "final_model_log_scaled.score(X_test_log_scaled, y_test_log_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find scaled RMSE\n",
    "RMSE_log_scaled = mean_squared_error(y_test_log_scaled,\n",
    "                   final_model_log_scaled.predict(X_test_log_scaled),\n",
    "                   squared=False)\n",
    "np.exp(RMSE_log_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Linear Assumptions (though not as important for predictive purposes)\n",
    "Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_model_log_scaled.predict(X_test_log_scaled)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(y_test_log_scaled.min(), y_test_log_scaled.max())\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"orange\", label=\"Perfect Fit\")\n",
    "ax.scatter(y_test_log_scaled+1.5, preds-2, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y_test_log_scaled - preds)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity (Independence Assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = [variance_inflation_factor(X_train_log_scaled.values, i) for i in range(X_train_log_scaled.shape[1])]\n",
    "pd.Series(vif, index=X_train_log_scaled.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(preds, residuals, alpha=0.5)\n",
    "ax.plot(preds, [0 for i in range(len(X_test_log_scaled))])\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate indpendent and dependent variables\n",
    "X = df.drop('price',axis=1)\n",
    "to_drop = ['id','date','lat','long','zipcode','yr_built','yr_renovated']\n",
    "X = X.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a first model using all relevant independent variables\n",
    "X = sm.add_constant(X)\n",
    "inf_model = sm.OLS(y, X)\n",
    "inf_model = inf_model.fit()\n",
    "inf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-values for sqft_lot, floors, and sqft_above are high which makes them not significant. We will remove those features. The Cond. No. is high as well, which means there is collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = preds = inf_model.predict(X)\n",
    "rmse = mean_squared_error(y, y_predicted, squared=False)\n",
    "print(f'rmse: {rmse}')\n",
    "print(f'R-squared: {inf_model.rsquared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by dropping the variables with high p-values and rerunning the model\n",
    "X = X.drop(['sqft_lot','floors','sqft_above'], axis=1)\n",
    "inf_model = sm.OLS(y, X)\n",
    "inf_model = inf_model.fit()\n",
    "inf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-values are good for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inf_model.predict(X)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(y.min(), y.max())\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"orange\", label=\"Perfect Fit\")\n",
    "ax.scatter(y, predictions, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = [variance_inflation_factor(X.values[1:], i) for i in range(X.shape[1])]\n",
    "list(zip(X.columns[1:], vif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = X.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "df_1['pairs'] = list(zip(df_1.level_0, df_1.level_1))\n",
    "\n",
    "df_1.set_index(['pairs'], inplace = True)\n",
    "\n",
    "df_1.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# cc for correlation coefficient\n",
    "df_1.columns = ['cc']\n",
    "\n",
    "df_1.drop_duplicates(inplace=True)\n",
    "\n",
    "df_1[(df_1.cc>.75) & (df_1.cc<1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the VIFs of the features, we have that bedrooms has a high VIF value which indicates multicollinearity. We also see that grade, sqft_living15, and bathrooms are all correlated with sqft_living."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Approach: Eliminate Outliers for price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot.hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of price is heavily skewed right. We will eliminate these outliers but limiting our price to $1,200,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe where price is less than or equal to $1,200,000\n",
    "new_df = df[df['price'] <= 1200000]\n",
    "new_y = new_df.price\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at new distribution of price\n",
    "new_y.plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price is still skewed but more normal than before. We do not want to lose too much data or that will affect our results negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = new_df.drop('price',axis=1)\n",
    "to_drop = ['id','date','lat','long','zipcode','yr_built','yr_renovated']\n",
    "new_X = new_X.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = sm.add_constant(new_X)\n",
    "inf_model = sm.OLS(new_y, new_X)\n",
    "inf_model = inf_model.fit()\n",
    "inf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inf_model.predict(new_X)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(new_y.min(), new_y.max())\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"orange\", label=\"Perfect Fit\")\n",
    "ax.scatter(new_y, predictions, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(inf_model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inf_model.predict(new_X), inf_model.resid)\n",
    "plt.plot(inf_model.predict(new_X), [0 for i in range(len(new_X))]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More refinements to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = new_X.drop('sqft_basement', axis=1)\n",
    "inf_model = sm.OLS(new_y, new_X)\n",
    "inf_model = inf_model.fit()\n",
    "inf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inf_model.predict(new_X)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(new_y.min(), new_y.max())\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"orange\", label=\"Perfect Fit\")\n",
    "ax.scatter(new_y, predictions, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(inf_model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inf_model.predict(new_X), inf_model.resid)\n",
    "plt.plot(inf_model.predict(new_X), [0 for i in range(len(new_X))]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = [variance_inflation_factor(new_X.values[1:], i) for i in range(new_X.shape[1])]\n",
    "list(zip(new_X.columns[1:], vif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = new_X.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "df_1['pairs'] = list(zip(df_1.level_0, df_1.level_1))\n",
    "\n",
    "df_1.set_index(['pairs'], inplace = True)\n",
    "\n",
    "df_1.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# cc for correlation coefficient\n",
    "df_1.columns = ['cc']\n",
    "\n",
    "df_1.drop_duplicates(inplace=True)\n",
    "\n",
    "df_1[(df_1.cc>.75) & (df_1.cc<1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_2 = new_X.drop(['sqft_above','bedrooms'],axis=1)\n",
    "inf_model = sm.OLS(new_y, new_X_2)\n",
    "inf_model = inf_model.fit()\n",
    "inf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inf_model.predict(new_X_2)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(new_y.min(), new_y.max())\n",
    "ax.plot(perfect_line, linestyle=\"--\", color=\"orange\", label=\"Perfect Fit\")\n",
    "ax.scatter(new_y, predictions, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(inf_model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inf_model.predict(new_X_2), inf_model.resid)\n",
    "plt.plot(inf_model.predict(new_X_2), [0 for i in range(len(new_X_2))]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(inf_model.predict(new_X_2),new_y)**.5\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
